{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10116468,"sourceType":"datasetVersion","datasetId":6241737},{"sourceId":10145273,"sourceType":"datasetVersion","datasetId":6262264},{"sourceId":10148162,"sourceType":"datasetVersion","datasetId":6264536},{"sourceId":10153278,"sourceType":"datasetVersion","datasetId":6268428},{"sourceId":10154006,"sourceType":"datasetVersion","datasetId":6268938},{"sourceId":10155407,"sourceType":"datasetVersion","datasetId":6269962},{"sourceId":10176258,"sourceType":"datasetVersion","datasetId":6285504},{"sourceId":10185815,"sourceType":"datasetVersion","datasetId":6292529},{"sourceId":10211735,"sourceType":"datasetVersion","datasetId":6311504},{"sourceId":10214156,"sourceType":"datasetVersion","datasetId":6313157},{"sourceId":10214317,"sourceType":"datasetVersion","datasetId":6313289},{"sourceId":10215283,"sourceType":"datasetVersion","datasetId":6314074},{"sourceId":10221430,"sourceType":"datasetVersion","datasetId":6318760},{"sourceId":10222640,"sourceType":"datasetVersion","datasetId":6319600},{"sourceId":10224457,"sourceType":"datasetVersion","datasetId":6320933},{"sourceId":10224583,"sourceType":"datasetVersion","datasetId":6321025},{"sourceId":10224866,"sourceType":"datasetVersion","datasetId":6321240},{"sourceId":10231164,"sourceType":"datasetVersion","datasetId":6325928},{"sourceId":10234746,"sourceType":"datasetVersion","datasetId":6328464},{"sourceId":10234760,"sourceType":"datasetVersion","datasetId":6328477},{"sourceId":10234768,"sourceType":"datasetVersion","datasetId":6328485},{"sourceId":10235219,"sourceType":"datasetVersion","datasetId":6328836},{"sourceId":10235311,"sourceType":"datasetVersion","datasetId":6328909},{"sourceId":10236077,"sourceType":"datasetVersion","datasetId":6329486},{"sourceId":10236216,"sourceType":"datasetVersion","datasetId":6329592},{"sourceId":10236365,"sourceType":"datasetVersion","datasetId":6329706},{"sourceId":10236440,"sourceType":"datasetVersion","datasetId":6329761},{"sourceId":10236483,"sourceType":"datasetVersion","datasetId":6329799},{"sourceId":10236585,"sourceType":"datasetVersion","datasetId":6329886},{"sourceId":10236869,"sourceType":"datasetVersion","datasetId":6330123},{"sourceId":10236878,"sourceType":"datasetVersion","datasetId":6330130},{"sourceId":10236972,"sourceType":"datasetVersion","datasetId":6330203},{"sourceId":10243167,"sourceType":"datasetVersion","datasetId":6334642},{"sourceId":10243319,"sourceType":"datasetVersion","datasetId":6334762},{"sourceId":10243329,"sourceType":"datasetVersion","datasetId":6334769},{"sourceId":10243464,"sourceType":"datasetVersion","datasetId":6334867},{"sourceId":10243495,"sourceType":"datasetVersion","datasetId":6334890},{"sourceId":10243992,"sourceType":"datasetVersion","datasetId":6335278},{"sourceId":10244042,"sourceType":"datasetVersion","datasetId":6335307},{"sourceId":10250325,"sourceType":"datasetVersion","datasetId":6340052},{"sourceId":10274227,"sourceType":"datasetVersion","datasetId":6357216},{"sourceId":10274315,"sourceType":"datasetVersion","datasetId":6357272},{"sourceId":10275630,"sourceType":"datasetVersion","datasetId":6358064},{"sourceId":10275711,"sourceType":"datasetVersion","datasetId":6358128},{"sourceId":10275777,"sourceType":"datasetVersion","datasetId":6358184},{"sourceId":10276019,"sourceType":"datasetVersion","datasetId":6358369},{"sourceId":10276057,"sourceType":"datasetVersion","datasetId":6358391},{"sourceId":10276468,"sourceType":"datasetVersion","datasetId":6358705},{"sourceId":10276978,"sourceType":"datasetVersion","datasetId":6359071},{"sourceId":10277116,"sourceType":"datasetVersion","datasetId":6359148},{"sourceId":10286418,"sourceType":"datasetVersion","datasetId":6365667},{"sourceId":10286422,"sourceType":"datasetVersion","datasetId":6365670},{"sourceId":10286434,"sourceType":"datasetVersion","datasetId":6365674},{"sourceId":10286441,"sourceType":"datasetVersion","datasetId":6365678}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction: Simplifying Dataset Preparation from Hard Copy Questions\n\nThis workflow is designed to help you transform questions from physical materials (like books or PDFs) into a well-organized digital dataset. Whether you're working with scanned documents or PDF files, this process ensures that the data is cleaned, structured, and ready for use without requiring advanced technical knowledge.\n\nBy using tools like **Tesseract-OCR** and **EasyOCR**, you can extract text accurately from images or PDFs. Additionally, **PyMuPDF** helps process PDF files efficiently. The steps are easy to follow, even for beginners, making it ideal for creating datasets on platforms like Kaggle.\n\n---\n\n## Workflow Overview\n\n### **Step 1–4: Question Cropping Workflow**\n\n1. **PDF to Image Conversion**  \n   - Convert each page of the PDF into an image.  \n   - This step simplifies processing and helps in removing parts you don’t need, such as headers or footers.\n\n2. **Removing Unnecessary Sections**  \n   - Identify and eliminate redundant parts like repeated titles or sections (e.g., \"Kertas 2\").  \n   - This ensures the dataset is focused on relevant questions.\n\n3. **Tag Cleanup**  \n   - Detect and remove irrelevant tags or extra text that might clutter your dataset.\n\n4. **Column & Question Cropping**  \n   - Split pages formatted in multiple columns (e.g., two or three columns).  \n   - Process each column separately to ensure clean and organized extraction of questions and content.\n\n---\n\n### **Step 5: Answer Extraction Workflow**\n\n5. **Answer Extraction**  \n   - Use OCR tools (like EasyOCR or Tesseract) to automatically detect and extract answer keys or solutions from the images.  \n   - Save extracted answers in a structured format (e.g., JSON), ensuring both question and answer data are preserved for further analysis.\n\n---\n\n### **Step 6–7: Dataset Consolidation Workflow**\n\n6. **Combine Questions and Answers**  \n   - Merge the datasets containing questions and answers into a unified JSON file.  \n   - Each entry in the JSON file will include fields for the question, its associated answer, and any relevant metadata (e.g., page or section number).\n\n7. **Export to Final Dataset**  \n   - Convert the consolidated JSON file into a user-friendly format (e.g., `.csv` or `.xlsx`).  \n   - This final dataset can be used for reviewing, sharing, or additional machine learning tasks.\n\n---\n\n## Summary of Outputs\n\n- **Step 1–4**: A set of cropped images containing only relevant questions, saved as PNG files.  \n- **Step 5**: A JSON file containing extracted answers, organized for each question.  \n- **Step 6–7**: A consolidated dataset with questions and answers in JSON, then exported to `.csv` format for end users.\n","metadata":{}},{"cell_type":"code","source":"# Install Tesseract OCR PyMuPDF\n!sudo apt-get install -y tesseract-ocr --quiet\n!pip install PyMuPDF --quiet\n!pip install --upgrade easyocr  --quiet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-24T02:52:34.982466Z","iopub.execute_input":"2024-12-24T02:52:34.983160Z","iopub.status.idle":"2024-12-24T02:52:56.188450Z","shell.execute_reply.started":"2024-12-24T02:52:34.983129Z","shell.execute_reply":"2024-12-24T02:52:56.187255Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Worflow for PDF to Image Conversion & Removing Unnecessary Sections\n\n### Workflow Summary Table\n\n| **Book Name**       | **Header Keyword**    | **Header Padding** | **Footer Keyword**               | **Footer Padding** | **Footer Adjustment**                                              |\n|----------------------|-----------------------|---------------------|-----------------------------------|---------------------|----------------------------------------------------------------------|\n| **FC065244 Book**    | `Praktis`            | 320                 | `\"Sasbadi Sdn\"`                  | 30                  | Use OCR to detect footer keyword; adjust crop if detected.           |\n| **FC064244 Book**    | `Praktis`            | 320                 | `\"Sasbadi Sdn\"`                  | 30                  | Use OCR to detect footer keyword; adjust crop if detected.           |\n| **QC174032 Book**    | `Ujian`              | 530                 | `\"Penerbitan Pelangi Sdn\"`       | 30                  | Use OCR to detect footer keyword; adjust crop if detected.           |\n| **KM24SF1 & KM24SMA Book** | `Kertas`      | 30                  | `KM`                             | 50                  | Look for footer keyword `KM` and adjust footer crop dynamically.     |\n| **GG24SFI4 Book**    | `Gerak`              | 320                 | Digit in last few footer texts   | Dynamic             | Use EasyOCR and Tesseract to find numbers in footer; apply dynamic padding if undetected. |\n| **IB4MA Book**       | `Bidang`             | 340                 | Digit in last few footer texts   | Dynamic             | Same workflow as GG24SFI4: Use OCR to detect numbers in footer and apply dynamic padding if undetected. |\n\n---\n\n### Header and Footer Extraction Process\n\n1. **Header Extraction**  \n   - Analyze the **top quarter** of the page for header text using OCR.  \n   - Detect and compare text against a customizable list of header keywords:  \n     `header_keywords = [\"Bidang\", \"Jawapan\", \"Praktis\", \"Fizik Tingkatan 5 Praktis\", \"Ujian\", \"Kertas Model\", \"Gerak\"]`  \n   - Determine the crop position based on the detected keywords:  \n     - `\"Bidang\"` → Set header crop to `340 + padding`.  \n     - `\"Kertas\"` → Set header crop to `30 + padding`.  \n     - No matching keyword → Do not crop the header.\n\n2. **Footer Extraction**  \n   - Analyze the **bottom 1/9 region** of the page using OCR.  \n   - Detect text against a customizable list of footer keywords:  \n     `footer_keywords = [\"KM\"]`  (can include `\"Penerbitan Pelangi Sdn\"` and `\"Sasbadi Sdn\"`).  \n   - If detected, adjust the footer crop dynamically.  \n   - For books like **IB4MA**, apply `footer_y = h - (-10) - padding` if no footer keyword or number is found.\n\n---\n\n### Parameters You Can Customize\n\n1. **Header Keywords**  \n   - Adjust the list of keywords to fit the type of document.  \n   - Example:  \n     `header_keywords = [\"Bidang\", \"Praktis\", \"Fizik Tingkatan 5 Praktis\", \"Kertas Model\", \"Gerak\"]`\n\n2. **Footer Keywords**  \n   - Modify the footer keyword list to detect specific text or publisher details.  \n   - Example:  \n     `footer_keywords = [\"KM\", \"Penerbitan Pelangi Sdn\", \"Sasbadi Sdn\"]`\n\n3. **Padding**  \n   - Configure the padding for cropping areas.  \n   - Example:  \n     - `header_padding = 340` (for larger header spaces)  \n     - `footer_padding = 30` (for smaller footer areas)\n\n4. **Dynamic Adjustment**  \n   - Adjust footer cropping dynamically using bounding box positions and additional offsets.\n\n---\n\n### Output\n\n1. **Processed Images**  \n   - Cropped images with detected keywords in the header or footer are saved in the output folder.\n\n2. **Fallback for Missing Keywords**  \n   - If no keywords are detected, the original image is saved without cropping.\n\n3. **Organized File Structure**  \n   - Images are saved with meaningful filenames, including chapter and page numbers for easy navigation.","metadata":{}},{"cell_type":"code","source":"import os\nimport fitz  # PyMuPDF\nimport cv2\nimport numpy as np\nfrom scipy.ndimage import rotate\nimport matplotlib.pyplot as plt\nimport pytesseract\nimport easyocr\nimport re\n\ndef pdf_to_images(pdf_path, output_folder):\n    \"\"\"\n    Converts each page of a PDF file to an image, processes each image,\n    and saves only the cropped output images to the output folder.\n    \n    Args:\n        pdf_path (str): The path to the PDF file.\n        output_folder (str): The folder where the cropped images will be saved.\n    \"\"\"\n    pdf_document = fitz.open(pdf_path)\n\n    # Extract base name and join it into a string\n    base_name = '_'.join(os.path.basename(pdf_path).split('_')[:-1])\n\n    # Define the dynamic output folder based on base_name\n    output_folder = f\"./output_final_images_{base_name}\"\n\n    if not os.path.exists(output_folder):\n        os.makedirs(output_folder)\n\n    # Extract chapter number from the PDF filename\n    chapter_match = re.search(r'_C(\\d+)', pdf_path)\n    if chapter_match:\n        chapter = int(chapter_match.group(1))\n\n    for page_number in range(pdf_document.page_count):\n        # Convert PDF page to image\n        page = pdf_document.load_page(page_number)\n        pix = page.get_pixmap(matrix=fitz.Matrix(4, 4))\n        \n        # Convert the page to a numpy array without saving initially\n        img_data = np.frombuffer(pix.samples, dtype=np.uint8)\n        img_data = img_data.reshape((pix.height, pix.width, pix.n))\n        image = cv2.cvtColor(img_data, cv2.COLOR_RGB2BGR)  # Convert RGB to BGR for OpenCV\n\n        # Plot and save the original image after converting from PDF\n        plt.figure(figsize=(10, 5))\n        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n        plt.title(f'Original Image - Page {page_number + 1}')\n        plt.axis('off')\n        plt.show()\n        \n        # Print detected text before cropping header and footer\n        full_text = easyocr_extract_text(image)\n        \n         # Crop content after skew correction        \n        cropped_image = crop_questions_and_answers(image) \n\n         # Crop header and footer if matching keywords are found\n        cropped_image = crop_header_footer_keywords(cropped_image) \n\n        # Apply line-based skew correction using the longest detected line to ensure everything is straight\n        _, line_corrected_image = correct_skew(cropped_image)\n\n        # Plot the final cropped image without header and footer, and with line correction\n        plt.figure(figsize=(10, 5))\n        plt.imshow(cv2.cvtColor(line_corrected_image, cv2.COLOR_BGR2RGB))\n        plt.title(f'Final Cropped Image (Line Corrected) - Page {page_number + 1}')\n        plt.axis('off')\n        plt.show()\n\n        # Save only the cropped image with updated chapter number\n        new_image_path = os.path.join(output_folder, f\"{base_name}_C{chapter}_P{page_number + 1}.png\")\n        cv2.imwrite(new_image_path, line_corrected_image)\n        print(f\"Corrected skew and cropped for page {page_number + 1} and saved: {new_image_path}\")\n\n    pdf_document.close()\n    \ndef correct_skew(image, delta=1, limit=5):\n    \"\"\"\n    Corrects the skew of an image.\n    Args:\n        image (numpy.ndarray): The input image to correct.\n        delta (int): The increment for the angle to test.\n        limit (int): The range of angles to test for skew correction.\n    Returns:\n        tuple: The best angle and the corrected image.\n    \"\"\"\n    def determine_score(arr, angle):\n        data = rotate(arr, angle, reshape=False, order=0)\n        histogram = np.sum(data, axis=1, dtype=float)\n        score = np.sum((histogram[1:] - histogram[:-1]) ** 2, dtype=float)\n        return histogram, score\n\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n\n    scores = []\n    angles = np.arange(-limit, limit + delta, delta)\n    for angle in angles:\n        _, score = determine_score(thresh, angle)\n        scores.append(score)\n\n    best_angle = angles[scores.index(max(scores))]\n\n    (h, w) = image.shape[:2]\n    center = (w // 2, h // 2)\n    M = cv2.getRotationMatrix2D(center, best_angle, 1.0)\n    corrected = cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n\n    return best_angle, corrected\n\n\ndef crop_questions_and_answers(image, margin=40, exclusion_threshold=0.1, min_text_area=10):\n    \"\"\"\n    Dynamically crop the image to retain only the question and answer sections.\n    Crops out areas without text and respects exclusion zones for the right and bottom sides.\n\n    Args:\n        image (numpy.ndarray): The input image.\n        margin (int): Additional padding to add around the cropped region.\n        exclusion_threshold (float): Proportion of the width considered as the exclusion zone on the right side.\n        min_text_area (int): Minimum area of a text region to be considered significant.\n\n    Returns:\n        numpy.ndarray: The cropped image.\n    \"\"\"\n    # Convert to grayscale and binary image\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    _, binary = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY_INV)\n\n    # Step 1: Detect text regions using EasyOCR\n    reader = easyocr.Reader(['en', 'ms'])\n    results = reader.readtext(image)\n\n    # Step 2: Filter text regions\n    significant_text_regions = []\n    image_width = image.shape[1]\n    image_height = image.shape[0]\n    right_exclusion_zone = image_width * (1 - exclusion_threshold)\n    bottom_exclusion_zone = image_height * (1 - exclusion_threshold)\n\n    for res in results:\n        ((x_min, y_min), _, (x_max, y_max), _) = res[0]\n        text_width = x_max - x_min\n        text_height = y_max - y_min\n        text_area = text_width * text_height\n\n        # Exclude text regions that fall within the exclusion zone\n        if (\n            text_area >= min_text_area\n            and x_max < right_exclusion_zone\n            and y_max < bottom_exclusion_zone\n        ):\n            significant_text_regions.append((x_min, y_min, x_max, y_max))\n\n    # Step 3: Determine cropping boundaries\n    if significant_text_regions:\n        x_min = min([region[0] for region in significant_text_regions])\n        y_min = min([region[1] for region in significant_text_regions])\n        x_max = max([region[2] for region in significant_text_regions])\n        y_max = max([region[3] for region in significant_text_regions])\n\n        # Apply margin\n        x_min = max(0, int(x_min) - margin)\n        y_min = max(0, int(y_min) - margin)\n        x_max = min(image.shape[1], int(x_max) + margin)\n        y_max = min(image.shape[0], int(y_max) + margin)\n\n        # Crop the image\n        cropped_image = image[y_min:y_max, x_min:x_max]\n\n        # Step 4: Additional crop for empty areas without text\n        binary_cropped = cv2.threshold(cv2.cvtColor(cropped_image, cv2.COLOR_BGR2GRAY), 240, 255, cv2.THRESH_BINARY_INV)[1]\n        row_sums = np.sum(binary_cropped, axis=1)\n        col_sums = np.sum(binary_cropped, axis=0)\n\n        # Detect empty areas (rows and columns with no text)\n        empty_top = np.where(row_sums > 0)[0][0] if np.any(row_sums > 0) else 0\n        empty_bottom = np.where(row_sums > 0)[0][-1] if np.any(row_sums > 0) else cropped_image.shape[0]\n        empty_left = np.where(col_sums > 0)[0][0] if np.any(col_sums > 0) else 0\n        empty_right = np.where(col_sums > 0)[0][-1] if np.any(col_sums > 0) else cropped_image.shape[1]\n\n        # Apply the additional cropping\n        cropped_image = cropped_image[empty_top:empty_bottom, empty_left:empty_right]\n\n        return cropped_image\n    else:\n        # If no significant text is detected, return the original image\n        return image\n\n\ndef crop_header_footer_keywords(image, padding=50):\n    \"\"\"\n    Crops the header and footer from an image if keywords are present.\n    Args:\n        image (numpy.ndarray): The input image to crop.\n        padding (int): The number of pixels to add around detected text regions.\n    Returns:\n        numpy.ndarray: The cropped image with header and footer removed if keywords match.\n    \"\"\"\n    header_keywords = [\"Bidang\",\"Jawapan\", \"Praktis\", \"Fizik Tingkatan 5 Praktis\", \"Ujian\", \"Kertas Model\", \"Gerak\" ]\n    footer_keywords = [ \"KM\"] #\"Penerbitan Pelangi Sdn\", \"Sasbadi Sdn\"\n\n    h, w = image.shape[:2]\n\n    # Convert image to grayscale\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    # Header cropping\n    # Limit header search area to the upper quarter of the page\n    header_area = gray[0:h//4, :]  \n    header_text = ' '.join(easyocr_extract_text(header_area).strip().lower().split()[:2])\n    # Praktis = 300 FC065244 , fizik 30 \n    # UJian = 530 QC174032, matematik 30\n    # Print the extracted text for debugging\n    print(f\"Detected header text: '{header_text}'\")\n    \n    # Logic for determining header crop position\n    if \"bidang\" in header_text:\n        header_y = 340 + padding\n        print(\"Detected 'Bidang' in header text. Setting header_y to:\", header_y)\n    elif \"kertas\" in header_text:\n        header_y = 30 + padding\n        print(\"Detected 'Kertas' in header text. Setting header_y to:\", header_y)\n    else:\n        header_y = 0\n        print(\"No matching keyword found in header text. Setting header_y to:\", header_y)\n\n\n    # Initialize OCR reader\n    reader = easyocr.Reader(['en', 'ms'])\n    \n     # ---------------- FOOTER CROPPING ----------------\n    footer_area = gray[h - (h // 9):h, :]  # Search area: bottom 1/8\n    \n    # EasyOCR extraction\n    footer_text_easyocr = ' '.join(reader.readtext(footer_area, detail=0)).lower()\n    # Tesseract extraction\n    footer_text_tesseract = pytesseract.image_to_string(footer_area, config='--psm 6').lower()\n    \n    # Combine results\n    footer_text = footer_text_easyocr + \" \" + footer_text_tesseract\n    footer_last_words = footer_text.split()[-5:]  # Take only the last 5 words for detection\n    print(f\"[EasyOCR + Tesseract] Last detected words in 1/8 region: '{' '.join(footer_last_words)}'\")  # Debugging print\n    \n    footer_y = h\n    # Step 1: Check for footer keywords or page numbers in bottom 1/8\n    if any(keyword.lower() in footer_text for keyword in footer_keywords):\n        footer_y = h - 30 - padding\n        print(\"Detected footer keyword in 1/8 region\")\n    elif any(re.search(r'\\b\\d+\\b', word) for word in footer_last_words):  \n        # Compare numeric page numbers from last few words\n        footer_y = h - (-10) - padding\n        print(f\"Detected page number in 1/8 region: '{' '.join(footer_last_words)}'\")\n    else:\n        footer_y = h - (-40) - padding\n        print(f\"Detected page number after dynamic padding: '{' '.join(footer_last_words)}'\")\n    \n    # Crop the image to remove the detected footer\n    cropped_image = image[header_y:footer_y, :]\n    return cropped_image\n\ndef easyocr_extract_text(image):\n    \"\"\"\n    Extract text from an image using EasyOCR.\n    Args:\n        image (numpy.ndarray): The input image to extract text from.\n    Returns:\n        str: The extracted text.\n    \"\"\"\n    reader = easyocr.Reader(['en', 'ms']) \n    results = reader.readtext(image, detail=0)\n    return ' '.join(results)\n\n# Process all PDF files in a folder\ninput_folder = \"/kaggle/input/ib4ma-pdf\"  \npdf_paths =  sorted([os.path.join(input_folder, file) for file in os.listdir(input_folder) if file.endswith(\".pdf\")])\n\n# Run the function to convert the PDF to images\n#pdf_paths = [\"/kaggle/input/qc174032/QC174032_C7.pdf\"]\noutput_folder = \"./output_final_images\"\nfor pdf_path in pdf_paths:\n    pdf_to_images(pdf_path, output_folder)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T06:18:16.118325Z","iopub.execute_input":"2024-12-17T06:18:16.118732Z","iopub.status.idle":"2024-12-17T06:30:17.744182Z","shell.execute_reply.started":"2024-12-17T06:18:16.118696Z","shell.execute_reply":"2024-12-17T06:30:17.743324Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Workflow: Detecting and Cropping \"Kertas 2\" or Similar Keywords from Images\n\n## Step-by-Step Workflow\n\n1. **Setup Input and Output Folders**\n   - Input images are stored in a folder (e.g., `/kaggle/working/output_final_images_IB4MA`).\n   - Processed images are saved to another folder (e.g., `/kaggle/working/processed_images_IB4MA_rK2`).\n\n2. **Keyword Detection with EasyOCR**\n   - EasyOCR is used to extract text from the image.\n   - The script searches for a main keyword (e.g., `\"Kertas\"`) and its associated numbers or text (e.g., `\"2\"`, `\"Bahagian A\"`).\n   - It checks up to **10 nearby words** (configurable via `proximity`) to match the associated keywords.\n\n3. **Crop Based on Detection**\n   - When a keyword (e.g., `\"Kertas 2\"`) is detected:\n     - The script retrieves the bounding box of the text and crops the image below the detected keyword, adding optional padding (e.g., `-70` for tighter crops).\n   - The cropped image is saved to the output folder.\n\n4. **Fallback for Undetected Keywords**\n   - If no keyword or associated text is found, the script saves the original image to the output folder without cropping.\n\n\n---\n\n## Parameters Can Customize\n- **Keyword**: The main word to detect in the image (default is `\"Kertas\"`).\n- **Keyword Numbers**: Additional associated text to search for (e.g., `[\"2\", \"Bahagian A\"]`).\n- **Padding**: Adjusts the crop area relative to the detected keyword (e.g., `-70` for tighter cropping).\n- **Proximity**: Number of nearby words to consider when searching for associated text (default is `10`).\n\n---\n## Output\n- Cropped images with detected keywords are saved to the output folder.\n- If no keywords are detected, the original image is saved without modification.\n\n","metadata":{}},{"cell_type":"code","source":"import cv2\nimport easyocr\nimport os\nimport matplotlib.pyplot as plt\n\n\ndef detect_and_crop_kertas2_easyocr(image_path, output_path, keyword=\"Kertas\", keyword_numbers=None, padding=20, proximity=10):\n    \"\"\"\n    Detects a keyword with specific associated keyword numbers (e.g., \"Kertas 2\" or \"Bahagian A\")\n    in an image and crops the bottom part of the image if found using EasyOCR.\n\n    Args:\n        image_path (str): Path to the input image.\n        output_path (str): Path to save the processed image.\n        keyword (str): The main keyword to detect in the image (e.g., \"Kertas\").\n        keyword_numbers (list): List of associated keywords or numbers (e.g., [\"2\", \"Bahagian A\"]).\n        padding (int): Additional padding to include when cropping.\n        proximity (int): Number of words to check near the detected keyword.\n\n    Returns:\n        None\n    \"\"\"\n    if keyword_numbers is None:\n        keyword_numbers = []\n\n    # Initialize EasyOCR reader\n    reader = easyocr.Reader(['en'], gpu=False)\n\n    # Load the main image\n    image = cv2.imread(image_path)\n    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    # Extract text using EasyOCR\n    results = reader.readtext(gray_image, detail=1)\n\n    # Debugging: Print all detected text with positions\n    print(f\"\\nProcessing Image: {os.path.basename(image_path)}\")\n\n    # Check for the keyword in the extracted text\n    for i, result in enumerate(results):\n        detected_text = result[1]\n        if keyword.lower() in detected_text.lower():\n            print(f\"\\nEasyOCR: Keyword '{keyword}' found.\")\n            # Print the next `proximity` words for debugging\n            next_words = [results[j][1] for j in range(i + 1, min(i + 1 + proximity, len(results)))]\n            print(f\"EasyOCR: Next {proximity} words: {next_words}\")\n\n            # Check for 'Bahagian A' or other keyword numbers\n            for key_num in keyword_numbers:\n                if key_num in next_words or \"bahagian a\" in \" \".join(next_words).lower():\n                    print(f\"EasyOCR: Detected '{keyword} {key_num}' or 'Bahagian A'.\")\n                    y = int(result[0][0][1]) \n                    cropped_image = image[:y + padding, :]  \n\n                    # Save the cropped image\n                    cv2.imwrite(output_path, cropped_image)\n                    print(f\"EasyOCR: Cropped and saved: {output_path}\")\n\n                    # Display the cropped image for verification\n                    plt.figure(figsize=(10, 5))\n                    plt.imshow(cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB))\n                    plt.title(f\"EasyOCR Cropped Image: {os.path.basename(output_path)}\")\n                    plt.axis(\"off\")\n                    plt.show()\n                    return\n\n    # If the keyword is not found, save the original image\n    cv2.imwrite(output_path, image)\n    print(f\"No keyword '{keyword}' or any of the keyword numbers {keyword_numbers} found by EasyOCR. Original image saved: {output_path}\")\n\n\n# Paths for processing\ninput_folder = \"/kaggle/working/output_final_images_IB4MA\"  \noutput_folder = \"/kaggle/working/processed_images_IB4MA_rK2\" \n\n# Ensure the output folder exists\nos.makedirs(output_folder, exist_ok=True)\n\n# List of keyword numbers (e.g., [\"2\", \"Bahagian A\"])\nkeyword_numbers_list = [\"2\", \"Bahagian A\"]\n\n# Custom image path for testing\ncustom_image_path = \"\"  \n\nif custom_image_path:\n    # Process only the custom image\n    custom_output_path = os.path.join(output_folder, os.path.basename(custom_image_path))\n    detect_and_crop_kertas2_easyocr(\n        image_path=custom_image_path,\n        output_path=custom_output_path,\n        keyword=\"Kertas\",  \n        keyword_numbers=keyword_numbers_list,  \n        padding=-60,  \n        proximity=10  \n    )\nelse:\n    # Process all images in the folder\n    for image_filename in sorted(os.listdir(input_folder)):\n        if image_filename.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n            input_image_path = os.path.join(input_folder, image_filename)\n            output_image_path = os.path.join(output_folder, image_filename)\n\n            # Run the function to detect and crop \"Kertas 2\" or other keyword numbers\n            detect_and_crop_kertas2_easyocr(\n                image_path=input_image_path,\n                output_path=output_image_path,\n                keyword=\"Kertas\",  \n                keyword_numbers=keyword_numbers_list,  \n                padding=-70,\n                proximity=10  \n            )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T06:36:14.992662Z","iopub.execute_input":"2024-12-17T06:36:14.993025Z","iopub.status.idle":"2024-12-17T06:53:40.058390Z","shell.execute_reply.started":"2024-12-17T06:36:14.992996Z","shell.execute_reply":"2024-12-17T06:53:40.057561Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Workflow: Removing Tags from Images with Template Matching\n\nThis workflow automates the process of detecting and removing unwanted tags (logos, watermarks, etc.) from images using **template matching** and inpainting.\n\n---\n\n## Step-by-Step Workflow\n\n1. **Set Input and Output Paths**\n   - **Input Folder**: Place the images you want to process in a folder (e.g., `/kaggle/input/fc065244-pdf-latest`).  \n   - **Templates Folder**: Place template images of the tags you want to remove in another folder (e.g., `/kaggle/input/remove-tags`).  \n   - **Output Folder**: Processed images will be saved in the specified output folder (e.g., `/kaggle/working/processed_images`).\n\n2. **Detect Tags**\n   - Each template in the **Templates Folder** is matched against the input images using **template matching**.  \n   - The algorithm identifies regions in the input image that closely resemble the template.\n\n3. **Remove Tags**\n   - Detected tag regions are inpainted by replacing them with a white rectangle.  \n   - The bounding box can be **shrunk or adjusted** to control the area of removal.\n\n4. **Save and Display Processed Images**\n   - The processed images are saved to the **Output Folder**.  \n   - The cropped or modified image is displayed for verification.\n\n---\n\n## Parameters You Can Customize\n\n1. **Similarity Threshold**  \n   - Controls how similar a detected region must be to the template to count as a match.  \n   - **Default**: `0.5` (values range from 0 to 1).  \n   - **Example**:  \n     - A higher value (e.g., `0.7`) makes the match stricter, reducing false positives.  \n     - A lower value (e.g., `0.3`) detects more regions but may include irrelevant areas.\n\n2. **Shrink Factor**  \n   - Shrinks the bounding box around the detected tag to avoid overlapping unnecessary regions.  \n   - **Default**: `5` (pixels).  \n   - **Example**:  \n     - A higher value reduces the tag area more aggressively.  \n     - A lower value keeps the bounding box closer to the original detected region.\n\n3. **Templates Directory**  \n   - The folder containing template images of the tags you want to remove.  \n   - **Example**: `/kaggle/input/remove-tags`.\n\n---\n\n## Output\n\n1. **Processed Images**  \n   - Images with detected tags removed are saved to the **Output Folder** (e.g., `/kaggle/working/processed_images`).\n\n2. **Fallback for Missing Tags**  \n   - If no tags are detected in an image, the original image is saved without modifications.\n\n3. **Display Processed Images**  \n   - The processed image is displayed for manual verification during the workflow.\n\n---\n\n## Example Usage\n- **Input Folder**: `/kaggle/input/fc065244-pdf-latest`  \n- **Templates Folder**: `/kaggle/input/remove-tags`  \n- **Output Folder**: `/kaggle/working/processed_images`\n\n---\n\n### For **Task 1: FC065244 Book**\n\n```python\noutput_dir = \"/kaggle/working/output_final_images_FC065244/cropped_question\"\nstitched_output_dir = \"/kaggle/working/output_final_images_FC065244/stitched_images\"\ndataset_folder = \"/kaggle/working/output_final_images_FC065244\"\n```\n\n---\n\n### For **Task 2: FC064244 Book**\n\n```python\noutput_dir = \"/kaggle/working/output_final_images_FC064244/cropped_question\"\nstitched_output_dir = \"/kaggle/working/output_final_images_FC064244/stitched_images\"\ndataset_folder = \"/kaggle/working/output_final_images_FC064244\"\n```\n\n---\n","metadata":{}},{"cell_type":"code","source":"import cv2\nimport os\n\ndef remove_tags(image_path, templates_dir, output_path, similarity_threshold=0.5, shrink_factor=0):\n    \"\"\"\n    Detects and removes multiple tags from an image using template matching and inpainting.\n\n    Args:\n        image_path (str): Path to the input image.\n        templates_dir (str): Directory containing template images of tags.\n        output_path (str): Path to save the processed image.\n        similarity_threshold (float): Threshold for template matching (0 to 1).\n        shrink_factor (int): Pixels to shrink bounding box on each side.\n\n    Returns:\n        None\n    \"\"\"\n    # Load the main image\n    image = cv2.imread(image_path)\n    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    # Iterate through all template images in the directory\n    for template_filename in os.listdir(templates_dir):\n        template_path = os.path.join(templates_dir, template_filename)\n        if template_filename.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n            # Load the template image\n            template = cv2.imread(template_path, 0)\n\n            # Template matching\n            result = cv2.matchTemplate(gray_image, template, cv2.TM_CCOEFF_NORMED)\n            min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n\n            # Process matches that exceed the similarity threshold\n            if max_val >= similarity_threshold:\n                top_left = max_loc\n                h, w = template.shape\n                bottom_right = (top_left[0] + w, top_left[1] + h)\n\n                # Shrink the bounding box slightly to avoid overlapping too much\n                top_left = (top_left[0] + shrink_factor, top_left[1] + shrink_factor)\n                bottom_right = (bottom_right[0] - shrink_factor, bottom_right[1] - shrink_factor)\n\n                cv2.rectangle(image, top_left, bottom_right, (255, 255, 255), -1)\n\n    # Save the result\n    cv2.imwrite(output_path, image)\n\n    # Display the processed image\n    plt.figure(figsize=(10, 5))\n    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    plt.title(f\"Processed Image: {os.path.basename(output_path)}\")\n    plt.axis(\"off\")\n    plt.show()\n\n# Paths for processing\ninput_folder = \"/kaggle/input/fc065244-pdf-latest\"\ntemplates_dir = \"/kaggle/input/remove-tags\"\noutput_folder = \"/kaggle/working/processed_images\"\n\n# Ensure the output folder exists\nos.makedirs(output_folder, exist_ok=True)\n\n# Process all images in the input folder\nfor image_filename in os.listdir(input_folder):\n    if image_filename.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n        input_image_path = os.path.join(input_folder, image_filename)\n        output_image_path = os.path.join(output_folder, image_filename)\n\n        # Run the function to remove tags\n        remove_tags(\n            image_path=input_image_path,\n            templates_dir=templates_dir,\n            output_path=output_image_path,\n            similarity_threshold=0.5,\n            shrink_factor=5\n        )\n\nprint(f\"Processed images are saved in: {output_folder}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T08:09:05.111388Z","iopub.status.idle":"2024-12-11T08:09:05.111674Z","shell.execute_reply.started":"2024-12-11T08:09:05.111541Z","shell.execute_reply":"2024-12-11T08:09:05.111555Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Workflow: Extracting and Organizing Questions from Images (3 Columns)\n\nThis workflow automates the process of extracting questions and their options from a set of images. It handles column-based layouts, detects questions, extracts relevant regions, and organizes them into structured outputs.\n\n---\n\n## Step-by-Step Workflow\n\n### 1. Organizing Input and Output Folders\n- **Input Folder**: Store images to process in the folder (e.g., `/kaggle/input/fc065244-image-completed`).\n- **Output Folders**:  \n  - `stitched_output_dir`: Saves vertically stitched column images.  \n  - `output_dir`: Saves cropped question images, organized by chapter and question number.\n\n---\n\n### 2. Stitched Column Layout\n- The image is divided into **three columns**.\n- These columns are stitched vertically into a single image for easier question extraction.\n\n---\n\n### 3. Detect and Extract Questions\n- Text is extracted using **pytesseract**, and numbered questions (e.g., `1.`, `2.`) are detected.\n- Options (`A, B, C, D`) and any text following the last option are also captured.\n- The algorithm ensures the **ordering of question numbers** is maintained:\n  - Skips duplicate or incorrectly ordered numbers.\n  - Accepts out-of-order numbers if the difference is within a reasonable range.\n\n---\n\n### 4. Cropping Questions and Options\n- For each detected question:\n  - The bounding box of the question and its associated options is determined.\n  - The region of interest (ROI) is cropped, including a padding of 15 pixels to capture context.\n- The cropped question images are saved in the **output folder**.\n\n---\n\n### 5. Stitching Across Pages\n- If a question is cut across two pages:\n  - The narrower image is padded with the background color to match widths.\n  - The question is stitched vertically to continue the context.\n\n---\n\n## Parameters You Can Customize\n\n1. **Paths**  \n   - `dataset_folder`: Folder containing input images.  \n   - `stitched_output_dir`: Folder to save stitched column images.  \n   - `output_dir`: Folder to save cropped questions.\n\n2. **Text Recognition Settings**  \n   - Uses pytesseract with `--psm 6` for OCR, optimized for multi-column text.\n\n3. **Padding**  \n   - Adjusts the padding around the cropped questions. Default: `15` pixels.\n\n4. **Symbols to Ignore**  \n   - Customize the list of ignored symbols or units during question detection.  \n   - Example: `ignore_symbols = [\"°\", \"×\", \"™\", \"%\", \"kg\", \"m\"]`.\n\n5. **Column Handling**  \n   - The image is divided into 3 columns by default.\n\n---\n\n## Output\n\n1. **Stitched Images**\n   - All columns in the input image are stitched vertically and saved to `stitched_output_dir`.\n\n2. **Cropped Questions**\n   - Each detected question and its options are saved as individual image files in `output_dir`.\n   - Images are named based on chapter and question number (e.g., `FC065244_C1_Q1.png`).\n\n3. **Unprocessed Images**\n   - If no questions are detected in an image, the original stitched image is saved for review.\n\n---\n\n## Example Usage\n\n### Input Files\n- Images: `/kaggle/input/fc065244-image-completed/FC065244_C1_P1.png`.\n\n### Output Files\n- **Stitched Images**:  \n  `/kaggle/working/FC065244_stitched_images/FC065244_C1_P1.png`.\n- **Cropped Questions**:  \n  `/kaggle/working/FC065244_cropped_questions_test4/FC065244_C1_Q1.png`","metadata":{}},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport pytesseract\nimport re\nimport os\nimport matplotlib.pyplot as plt\n\n# Define paths\noutput_dir = \"/kaggle/working/FC065244_cropped_questions_test4\"\nstitched_output_dir = \"/kaggle/working/FC065244_stitched_images\"\ndataset_folder = '/kaggle/input/fc065244-image-completed'\ncustom_image_path = \"\"  # Optional: specify a single image for processing\nos.makedirs(output_dir, exist_ok=True)\nos.makedirs(stitched_output_dir, exist_ok=True)\n\n\ndef extract_numbered_list(text):\n    \"\"\"\n    Extracts numbered lists (e.g., \"1.\", \"2,\", etc.) from a given text while ignoring unwanted symbols or units.\n    \"\"\"\n    ignore_symbols = [\"°\", \"•\", \"×\", \"+\", \"™\", \"©\", \"®\", \"%\", \"$\", \"€\", \"¥\", \"N\", \"kg\", \"m\"]\n    ignore_patterns = r\"|\".join(map(re.escape, ignore_symbols))\n    pattern = rf'(^|\\s)(\\d+)[\\.,](?![\\d{ignore_patterns}])'\n    matches = re.findall(pattern, text)\n\n    return [match[1] for match in matches if not re.search(ignore_patterns, text) and match[1] != \"0\"]\n\n\ndef crop_and_stitch_columns(image):\n    \"\"\"\n    Splits the image into three equal columns, then vertically stitches them into one image.\n    \"\"\"\n    height, width = image.shape[:2]\n    column_width = width // 3\n    columns = [image[:, start_x:start_x + column_width] for start_x in range(0, width, column_width)]\n\n    # Equalize column widths and stitch vertically\n    max_width = max(col.shape[1] for col in columns)\n    for i, col in enumerate(columns):\n        if col.shape[1] < max_width:\n            padding = np.zeros((col.shape[0], max_width - col.shape[1], 3), dtype=np.uint8)\n            columns[i] = np.hstack((col, padding))\n\n    return np.vstack(columns)\n\n\ndef get_text_and_boxes(image):\n    \"\"\"\n    Extracts text and bounding boxes from an image using pytesseract.\n    \"\"\"\n    data = pytesseract.image_to_data(image, config='--psm 6', output_type=pytesseract.Output.DICT)\n    return [(data['text'][i].strip(), (data['left'][i], data['top'][i], data['width'][i], data['height'][i]))\n            for i in range(len(data['text'])) if data['text'][i].strip()]\n\n\ndef detect_and_extract_questions_with_options(image):\n    \"\"\"\n    Detects questions and their options from an image, ensuring strict ordering of question numbers.\n    \"\"\"\n    text_boxes = get_text_and_boxes(image)\n    questions_with_options = []\n    current_question = None\n    current_options = []\n    after_options_text = []\n    option_labels = [\"A\", \"B\", \"C\", \"D\"]\n    detected_last_option = False\n    last_detected_question_number = 0\n\n    for text, (x, y, w, h) in text_boxes:\n        numbered_list = extract_numbered_list(text)\n        if numbered_list:\n            current_number = int(numbered_list[0])\n\n            # Validate question order\n            if current_number <= last_detected_question_number and (last_detected_question_number - current_number) > 1:\n                continue\n            last_detected_question_number = current_number\n\n            # Finalize the previous question\n            if current_question:\n                questions_with_options.append((current_question, current_options, after_options_text))\n            current_question = (current_number, (x, y, w, h))\n            current_options = []\n            after_options_text = []\n            detected_last_option = False\n\n        elif current_question:\n            # Check if the text is part of the options\n            qx, qy, qw, qh = current_question[1]\n            if y > qy and abs(y - (qy + qh)) < 3000:\n                if text in option_labels and not detected_last_option:\n                    current_options.append((text, (x, y, w, h)))\n                    if text == \"D\":\n                        detected_last_option = True\n                elif detected_last_option:\n                    after_options_text.append((text, (x, y, w, h)))\n\n    # Append the last question if needed\n    if current_question:\n        questions_with_options.append((current_question, current_options, after_options_text))\n\n    cropped_questions = []\n    for question, options, after_texts in questions_with_options:\n        question_number, (qx, qy, qw, qh) = question\n        min_x, min_y = qx, qy\n        max_x, max_y = qx + qw, qy + qh\n\n        for _, (x, y, w, h) in options + after_texts:\n            min_x, min_y, max_x, max_y = min(min_x, x), min(min_y, y), max(max_x, x + w), max(max_y, y + h)\n\n        padding = 15\n        roi = image[max(min_y - padding, 0):min(max_y + padding, image.shape[0]),\n                    max(min_x - padding, 0):min(max_x + padding, image.shape[1])]\n\n        cropped_questions.append((roi, question_number, [opt[0] for opt in options]))\n\n    return cropped_questions\n\n\ndef pad_to_match_width(image1, image2):\n    \"\"\"\n    Pads the narrower image to match the width of the wider image using its dominant background color.\n    \"\"\"\n    height1, width1 = image1.shape[:2]\n    height2, width2 = image2.shape[:2]\n\n    if width1 < width2:\n        padding = np.full((height1, width2 - width1, 3), np.mean(image1[:, :5], axis=(0, 1)), dtype=np.uint8)\n        return np.hstack((image1, padding)), image2\n    elif width2 < width1:\n        padding = np.full((height2, width1 - width2, 3), np.mean(image2[:, :5], axis=(0, 1)), dtype=np.uint8)\n        return image1, np.hstack((image2, padding))\n    return image1, image2\n\n\ndef main():\n    stitched_images_by_chapter = {}\n\n    # Organize images by chapter\n    image_files = sorted([f for f in os.listdir(dataset_folder) if f.endswith(\".png\")],\n                         key=lambda x: (x.split('_')[1], int(re.search(r'P(\\d+)', x).group(1))))\n\n    for image_file in image_files:\n        chapter_key = image_file.split('_')[1]\n        image_path = os.path.join(dataset_folder, image_file)\n        image = cv2.imread(image_path)\n        stitched_image = crop_and_stitch_columns(image)\n\n        if chapter_key not in stitched_images_by_chapter:\n            stitched_images_by_chapter[chapter_key] = []\n        stitched_images_by_chapter[chapter_key].append((stitched_image, image_file))\n\n    # Process each chapter\n    for chapter_key, chapter_images in stitched_images_by_chapter.items():\n        last_cropped_question = None\n\n        for i, (stitched_image, image_file) in enumerate(chapter_images):\n            if last_cropped_question:\n                stitched_image, last_cropped_question = pad_to_match_width(stitched_image, last_cropped_question)\n                stitched_image = np.vstack((last_cropped_question, stitched_image))\n\n            stitched_image_path = os.path.join(stitched_output_dir, f\"{image_file}\")\n            cv2.imwrite(stitched_image_path, stitched_image)\n\n            question_regions = detect_and_extract_questions_with_options(stitched_image)\n\n            for roi, question_number, _ in question_regions:\n                cropped_question_path = os.path.join(output_dir, f\"FC065244_{chapter_key}_Q{question_number}.png\")\n                cv2.imwrite(cropped_question_path, roi)\n\n            if question_regions:\n                last_cropped_question, _, _ = question_regions[-1]\n            else:\n                last_cropped_question = None\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Workflow: Extracting and Organizing Questions from Images (2 Columns)\n\nThis workflow automates the process of extracting questions and their options from a set of images. It handles **two-column layouts** in the same way as the three-column workflow, detects questions, extracts relevant regions, and organizes them into structured outputs.\n\n---\n\n### Example Usage\nTo update the output paths and dataset folder for the books in question, you can use the following template. Replace the placeholders with the relevant book names.\n\n---\n\n### Example Usage for Each Book\n\nFor **Task 3: QC174032 Book**\n\n```python\noutput_dir = \"/kaggle/working/output_final_images_QC174032/cropped_question\"\nstitched_output_dir = \"/kaggle/working/output_final_images_QC174032/stitched_images\"\ndataset_folder = \"/kaggle/working/output_final_images_QC174032\"\n```\n\n---\n\nFor **Task 4: KM24SF1 Book**\n\n```python\noutput_dir = \"/kaggle/working/output_final_images_KM24SF1/cropped_question\"\nstitched_output_dir = \"/kaggle/working/output_final_images_KM24SF1/stitched_images\"\ndataset_folder = \"/kaggle/working/output_final_images_KM24SF1\"\n```\n\n---\n\nFor **Task 5: KM24SMA Book**\n\n```python\noutput_dir = \"/kaggle/working/output_final_images_KM24SMA/cropped_question\"\nstitched_output_dir = \"/kaggle/working/output_final_images_KM24SMA/stitched_images\"\ndataset_folder = \"/kaggle/working/output_final_images_KM24SMA\"\n```\n\n---\n\nFor **Task 6: GG24SFI4 Book**\n\n```python\noutput_dir = \"/kaggle/working/output_final_images_GG24SFI4/cropped_question\"\nstitched_output_dir = \"/kaggle/working/output_final_images_GG24SFI4/stitched_images\"\ndataset_folder = \"/kaggle/working/output_final_images_GG24SFI4\"\n```\n\n---\n\nFor **Task 7: IB4MA Book**\n\n```python\noutput_dir = \"/kaggle/working/output_final_images_IB4MA/cropped_question\"\nstitched_output_dir = \"/kaggle/working/output_final_images_IB4MA/stitched_images\"\ndataset_folder = \"/kaggle/working/output_final_images_IB4MA\"\n```\n\n---","metadata":{}},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport pytesseract\nimport re\nimport os\nimport matplotlib.pyplot as plt\nimport fnmatch\n\n# Define paths\noutput_dir = \"/kaggle/working/output_final_images_QC174032/cropped_question\"\nstitched_output_dir = \"/kaggle/working/output_final_images_QC174032/stitched_images\"\ndataset_folder = '/kaggle/working/output_final_images_QC174032'\nos.makedirs(output_dir, exist_ok=True)\nos.makedirs(stitched_output_dir, exist_ok=True)\n\ndef extract_numbered_list(text):\n    \"\"\"\n    Extracts numbered questions (e.g., \"1.\", \"2.\") from a given text while ignoring unwanted symbols or units.\n    \"\"\"\n    ignore_symbols = [\"°\", \"•\", \"×\", \"+\", \"™\", \"©\", \"®\", \"%\", \"$\", \"€\", \"¥\", \"N\", \"kg\", \"m\"]\n    ignore_patterns = r\"|\".join(map(re.escape, ignore_symbols))\n    pattern = rf'(^|\\s)(\\d+)\\.(?![\\d{ignore_patterns}])'\n    matches = re.findall(pattern, text)\n    return [match[1] for match in matches if not re.search(ignore_patterns, text) and match[1] != \"0\"]\n\ndef tesseract_extract_text(image):\n    config = '--psm 6'\n    return pytesseract.image_to_string(image, config=config)\n\ndef crop_and_stitch_columns(image):\n    \"\"\"\n    Stitches two columns of an image vertically.\n    \"\"\"\n    column_width = image.shape[1] // 2\n    columns = [\n        image[:, :column_width],\n        image[:, column_width:]\n    ]\n    max_width = max(col.shape[1] for col in columns)\n    for i in range(len(columns)):\n        current_width = columns[i].shape[1]\n        if current_width < max_width:\n            padding = np.zeros((columns[i].shape[0], max_width - current_width, 3), dtype=np.uint8)\n            columns[i] = np.hstack((columns[i], padding))\n    return np.vstack(columns)\n\ndef get_text_and_boxes(image):\n    \"\"\"\n    Extracts text and bounding boxes from the image using pytesseract.\n    \"\"\"\n    data = pytesseract.image_to_data(image, config='--psm 6', output_type=pytesseract.Output.DICT)\n    return [(data['text'][i].strip(), (data['left'][i], data['top'][i], data['width'][i], data['height'][i]))\n            for i in range(len(data['text'])) if data['text'][i].strip()]\n\ndef detect_and_extract_questions_with_options(image):\n    \"\"\"\n    Detects and extracts questions and their options from the image.\n    \"\"\"\n    text_boxes = get_text_and_boxes(image)\n    questions_with_options = []\n    current_question = None\n    current_options = []\n    after_options_text = []\n    option_labels = [\"A\", \"B\", \"C\", \"D\"]\n    detected_last_option = False\n    skip_current_question = False\n    expected_question_number = None\n\n    for text, (x, y, w, h) in text_boxes:\n        if skip_current_question:\n            skip_current_question = False\n            continue\n        numbered_list = extract_numbered_list(text)\n        if numbered_list:\n            current_number = int(numbered_list[0])\n            if expected_question_number is not None and current_number != expected_question_number:\n                continue\n            expected_question_number = current_number + 1\n            if current_question:\n                questions_with_options.append((current_question, current_options, after_options_text))\n            current_question = (numbered_list[0], (x, y, w, h))\n            current_options = []\n            after_options_text = []\n            detected_last_option = False\n            skip_current_question = False\n        elif current_question:\n            qx, qy, qw, qh = current_question[1]\n            if y > qy and abs(y - (qy + qh)) < 2000:\n                if text in option_labels and not detected_last_option:\n                    current_options.append((text, (x, y, w, h)))\n                    if text == \"D\":\n                        detected_last_option = True\n                elif detected_last_option:\n                    if re.match(r'^[\\w\\s\\.,!?\\'\"-]*$', text) and len(text) > 1:\n                        if after_options_text and abs(y - after_options_text[-1][1][1]) > 100:\n                            skip_current_question = True\n                            continue\n                        after_options_text.append((text, (x, y, w, h)))\n                    else:\n                        current_options.append((text, (x, y, w, h)))\n\n    if current_question:\n        questions_with_options.append((current_question, current_options, after_options_text))\n\n    cropped_questions = []\n    for question, options, after_texts in questions_with_options:\n        question_number, (qx, qy, qw, qh) = question\n        min_x, min_y = qx, qy\n        max_x, max_y = qx + qw, qy + qh\n        for _, (x, y, w, h) in options + after_texts:\n            min_x, min_y, max_x, max_y = min(min_x, x), min(min_y, y), max(max_x, x + w), max(max_y, y + h)\n        padding = 15\n        roi = image[max(min_y - padding, 0):min(max_y + padding, image.shape[0]),\n                    max(min_x - padding, 0):min(max_x + padding, image.shape[1])]\n        cropped_questions.append((roi, question_number, [opt[0] for opt in options]))\n    return cropped_questions\n\ndef main():\n    image_files = sorted(\n        [f for f in os.listdir(dataset_folder) if fnmatch.fnmatch(f, \"QC174032_7_*.png\")]\n    )\n    if not image_files:\n        print(f\"No files matching the pattern 'QC174032_7_*.png' found in {dataset_folder}.\")\n        return\n    for image_file in image_files:\n        image_path = os.path.join(dataset_folder, image_file)\n        image = cv2.imread(image_path)\n        stitched_image = crop_and_stitch_columns(image)\n        stitched_image_path = os.path.join(stitched_output_dir, f\"stitched_{image_file}\")\n        cv2.imwrite(stitched_image_path, stitched_image)\n        question_regions = detect_and_extract_questions_with_options(stitched_image)\n        for i, (roi, question_number, options) in enumerate(question_regions):\n            cropped_question_path = os.path.join(output_dir, f\"{image_file}_question_{question_number}.png\")\n            cv2.imwrite(cropped_question_path, roi)\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T08:09:05.118312Z","iopub.status.idle":"2024-12-11T08:09:05.118603Z","shell.execute_reply.started":"2024-12-11T08:09:05.118462Z","shell.execute_reply":"2024-12-11T08:09:05.118476Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Another better version to crop Question for 2 column. Solved the numbered list problem ","metadata":{}},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport pytesseract\nimport re\nimport os\nimport matplotlib.pyplot as plt\n\n# Define paths\noutput_dir = \"/kaggle/working/IB4MA_cropped_questions_final\"\nstitched_output_dir = \"/kaggle/working/IB4MA_stitched_images\"\ndataset_folder = '/kaggle/working/processed_images_IB4MA_rK2'\nos.makedirs(output_dir, exist_ok=True) \nos.makedirs(stitched_output_dir, exist_ok=True)\n\ndef extract_numbered_list(text):\n    \"\"\"\n    Extracts numbers from text where the text contains only digits (e.g., '1', '23', '456').\n    \"\"\"\n    numbered_list = []\n    if re.match(r'^\\d+$', text):  \n        numbered_list.append(text)  # Add the detected number\n    return numbered_list\n\ndef crop_and_stitch_columns(image):\n    \"\"\"\n    Splits the image into two columns and stitches them vertically.\n    \"\"\"\n    height, width = image.shape[:2]\n    column_width = width // 2\n    columns = [(0, column_width), (column_width, width)]\n    column_images = []\n\n    for start_x, end_x in columns:\n        column_image = image[:, start_x:end_x]\n        column_images.append(column_image)\n\n    max_width = max(col.shape[1] for col in column_images)\n    for i in range(len(column_images)):\n        current_width = column_images[i].shape[1]\n        if current_width < max_width:\n            padding = np.zeros((column_images[i].shape[0], max_width - current_width, 3), dtype=np.uint8)\n            column_images[i] = np.hstack((column_images[i], padding))\n\n    stitched_image = np.vstack(column_images)\n    return stitched_image\n\ndef get_text_and_boxes(image):\n    \"\"\"\n    Extracts text and bounding boxes from the entire image using pytesseract.\n    \"\"\"\n    data = pytesseract.image_to_data(image, config='--psm 6', output_type=pytesseract.Output.DICT)\n    text_boxes = []\n    for i in range(len(data['text'])):\n        text = data['text'][i].strip()\n        if text:  # Ignore empty text\n            x, y, w, h = data['left'][i], data['top'][i], data['width'][i], data['height'][i]\n            text_boxes.append((text, (x, y, w, h)))\n    return text_boxes\n\ndef detect_and_extract_questions_with_options(image):\n    \"\"\"\n    Detects and extracts questions and options from the image.\n    \"\"\"\n    text_boxes = get_text_and_boxes(image)\n    questions_with_options = []\n    current_question = None\n    current_options = []\n    after_options_text = []\n    option_labels = [\"A\", \"B\", \"C\", \"D\"]\n    detected_last_option = False\n    left_threshold = 0.1\n    last_detected_question_number = 0\n    image_width = image.shape[1]\n\n    for text, (x, y, w, h) in text_boxes:\n        numbered_list = extract_numbered_list(text)\n        if numbered_list:\n            current_number = int(numbered_list[0])\n            if x > image_width * left_threshold:\n                continue          \n            if current_number <= last_detected_question_number and (last_detected_question_number - current_number) > 1:\n                continue\n            last_detected_question_number = current_number\n\n            if current_question is not None:\n                questions_with_options.append((current_question, current_options, after_options_text))\n\n            current_question = (numbered_list[0], (x, y, w, h))\n            current_options = []\n            after_options_text = []\n            detected_last_option = False\n\n        elif current_question is not None:\n            qx, qy, qw, qh = current_question[1]\n            if y > qy and abs(y - (qy + qh)) < 3000:\n                if text in option_labels and not detected_last_option:\n                    current_options.append((text, (x, y, w, h)))\n                    if text == \"D\":\n                        detected_last_option = True\n                elif detected_last_option:\n                    if re.match(r'^[\\w\\s\\.,]*$', text) and len(text) > 1:\n                        after_options_text.append((text, (x, y, w, h)))\n\n    if current_question is not None:\n        questions_with_options.append((current_question, current_options, after_options_text))\n\n    cropped_questions = []\n    for question, options, after_texts in questions_with_options:\n        question_number, (qx, qy, qw, qh) = question\n        min_x, min_y = qx, qy\n        max_x, max_y = qx + qw, qy + qh\n\n        for _, (x, y, w, h) in options + after_texts:\n            min_x, min_y = min(min_x, x), min(min_y, y)\n            max_x, max_y = max(max_x, x + w), max(max_y, y + h)\n\n        padding = 50\n        y_padded = max(min_y - padding, 0)\n        x_padded = max(min_x - padding, 0)\n        cropped_questions.append((image[y_padded:max(max_y + padding, image.shape[0]), \n                                        x_padded:max(max_x + padding, image.shape[1])], question_number, [opt[0] for opt in options]))\n    return cropped_questions\n\ndef pad_to_match_width(image1, image2):\n    \"\"\"\n    Pads the narrower image to match the width of the wider image.\n    \"\"\"\n    height1, width1 = image1.shape[:2]\n    height2, width2 = image2.shape[:2]\n    white_bg_color = (255, 255, 255)\n\n    if width1 < width2:\n        padding = np.full((height1, width2 - width1, 3), white_bg_color, dtype=np.uint8)\n        return np.hstack((image1, padding)), image2\n    elif width2 < width1:\n        padding = np.full((height2, width1 - width2, 3), white_bg_color, dtype=np.uint8)\n        return image1, np.hstack((image2, padding))\n    return image1, image2\n\ndef main():\n    stitched_images_by_chapter = {}\n    image_files = sorted(\n        [f for f in os.listdir(dataset_folder) if f.endswith(\".png\")], \n        key=lambda x: (\n            x.split('_')[1],\n            int(re.search(r'P(\\d+)', x).group(1)) if re.search(r'P(\\d+)', x) else 0\n        )\n    )\n\n    for image_file in image_files:\n        chapter_key = image_file.split('_')[1]\n        image_path = os.path.join(dataset_folder, image_file)\n        image = cv2.imread(image_path)\n        stitched_image = crop_and_stitch_columns(image)\n\n        if chapter_key not in stitched_images_by_chapter:\n            stitched_images_by_chapter[chapter_key] = []\n        stitched_images_by_chapter[chapter_key].append((stitched_image, image_file))\n\n    for chapter_key, chapter_images in stitched_images_by_chapter.items():\n        last_cropped_question = None\n\n        for i, (stitched_image, image_file) in enumerate(chapter_images):\n            if last_cropped_question is not None:\n                stitched_image, last_cropped_question = pad_to_match_width(stitched_image, last_cropped_question)\n                stitched_image = np.vstack((last_cropped_question, stitched_image))\n\n            stitched_image_path = os.path.join(stitched_output_dir, f\"{image_file}\")\n            cv2.imwrite(stitched_image_path, stitched_image)\n\n            question_regions = detect_and_extract_questions_with_options(stitched_image)\n\n            for roi, question_number, _ in question_regions:\n                cropped_question_path = os.path.join(output_dir, f\"IB4MA_{chapter_key}_Q{question_number}.png\")\n                cv2.imwrite(cropped_question_path, roi)\n\n            if question_regions:\n                last_cropped_question, _, _ = question_regions[-1]\n            else:\n                last_cropped_question = None\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Answer Extraction with OCR\r\nThis process uses **Tesseract** and **EasyOCR** to extract answers from images containing multiple-choice answers. It is designed specifically for images with a **single-column layout**, as multi-column formats may cause errors during text extraction.-\r\n\r\n## Process Overview\r\n1. **Input Requirements**:\r\n   - The script processes images in a structured, single-column format.\r\n   - Multi-column layouts are not supported without additional preprocessing.\r\n2. **OCR Methods**:\r\n   - Both **Tesseract** and **EasyOCR** are used for extracting text.\r\n   - Results are compared, and conflicts are resolved by prioritizing EasyOCR for better accuracy.\r\n3. **Output**:\r\n   - Extracted answers are saved in a structured format for urthr use.\r\n\r\n---\r\n\r\n## Supported Format (Single Column)\r\nExample:  \r\n`/kaggle/input/answer-extraction-instruction/FC4_C2_ANS.PNG`\r\n\r\n---\r\n\r\n## Unsupported Format (Two Columns)\r\nExample:  \r\n`/kaggle/input/answer-extraction-instructin/FC065244_C2_ANS_1.png`\r\n","metadata":{}},{"cell_type":"code","source":"import cv2\nimport pytesseract\nimport re\nimport easyocr\nimport numpy as np\nimport json\nfrom collections import defaultdict\nimport os\n\ndef preprocess_image(image):\n    # Preprocess the image to enhance OCR accuracy: resizing, grayscale conversion, blurring, sharpening, and thresholding.\n    image = cv2.resize(image, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n    sharpened = cv2.filter2D(blurred, -1, kernel)\n    _, thresh = cv2.threshold(sharpened, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n    return thresh\n\ndef tesseract_extract_text(image):\n    # Extract text using Tesseract OCR.\n    data = pytesseract.image_to_data(image, config='--psm 6', output_type=pytesseract.Output.DICT)\n    extracted_text = \" \".join([data['text'][i] for i in range(len(data['text'])) if int(data['conf'][i]) > 0])\n    return extracted_text\n\ndef easyocr_extract_text(image):\n    # Extract text using EasyOCR.\n    reader = easyocr.Reader(['en', 'ms'])\n    results = reader.readtext(image, detail=0, contrast_ths=0.7, text_threshold=0.6, low_text=0.5)\n    return ' '.join(results)\n\ndef extract_answers(text):\n    # Extract answers in the format 'number. answer' (e.g., '1. A').\n    answer_pattern = re.compile(r'(\\d+)\\.\\s*([A-D])')\n    return answer_pattern.findall(text)\n\ndef compare_ocr_methods(image_path):\n    # Compare the results of Tesseract and EasyOCR, and resolve conflicts by using EasyOCR answers when discrepancies are found.\n    image = cv2.imread(image_path)\n    preprocessed_image = preprocess_image(image)\n\n    tesseract_text = tesseract_extract_text(preprocessed_image)\n    tesseract_answers = extract_answers(tesseract_text)\n\n    easyocr_text = easyocr_extract_text(preprocessed_image)\n    easyocr_answers = extract_answers(easyocr_text)\n\n    answer_dict = defaultdict(lambda: {\"Tesseract\": None, \"EasyOCR\": None})\n    for number, answer in tesseract_answers:\n        answer_dict[number][\"Tesseract\"] = answer\n\n    for number, answer in easyocr_answers:\n        if answer_dict[number][\"Tesseract\"] is None:\n            answer_dict[number][\"EasyOCR\"] = answer\n        elif answer_dict[number][\"Tesseract\"] != answer:\n            answer_dict[number][\"EasyOCR\"] = answer\n\n    print(f\"Results for {image_path}:\")\n    print(f\"Tesseract OCR Results: {tesseract_text}\")\n    print(f\"EasyOCR Results: {easyocr_text}\")\n    print(\"Combined Results (Sorted):\")\n\n    missing_questions = []\n    conflicting_questions = []\n\n    max_question = max(map(int, answer_dict.keys())) if answer_dict else 0\n    for i in range(1, max_question + 1):\n        str_i = str(i)\n        if str_i not in answer_dict:\n            missing_questions.append(i)\n        else:\n            tesseract_answer = answer_dict[str_i][\"Tesseract\"]\n            easyocr_answer = answer_dict[str_i][\"EasyOCR\"]\n\n            if tesseract_answer and easyocr_answer and tesseract_answer != easyocr_answer:\n                conflicting_questions.append(i)\n                print(f\"Question {i}: Conflicting Answers Detected: Tesseract: {tesseract_answer}, EasyOCR: {easyocr_answer} (Using EasyOCR)\")\n                answer_dict[str_i][\"FinalAnswer\"] = easyocr_answer\n            elif tesseract_answer:\n                print(f\"Question {i}: Answer {tesseract_answer} (Detected by Tesseract)\")\n                answer_dict[str_i][\"FinalAnswer\"] = tesseract_answer\n            elif easyocr_answer:\n                print(f\"Question {i}: Answer {easyocr_answer} (Detected by EasyOCR)\")\n                answer_dict[str_i][\"FinalAnswer\"] = easyocr_answer\n\n    if missing_questions:\n        print(f\"\\nMissing Questions: {', '.join(map(str, missing_questions))}\")\n    if conflicting_questions:\n        print(f\"\\nConflicting Questions: {', '.join(map(str, conflicting_questions))} (Needs Recheck)\")\n    print(\"\\n\" + \"=\"*50 + \"\\n\")\n\n    for i in missing_questions:\n        print(f\"Rechecking missing question {i}...\")\n        str_i = str(i)\n\n        alternative_pattern = re.compile(fr'{i}\\s*([A-D])')\n        alternative_match = alternative_pattern.search(tesseract_text)\n        if alternative_match:\n            tesseract_answer = alternative_match.group(1)\n            answer_dict[str_i][\"Tesseract\"] = tesseract_answer\n            answer_dict[str_i][\"FinalAnswer\"] = tesseract_answer\n            print(f\"Question {i}: Found answer {tesseract_answer} in Tesseract recheck\")\n            continue\n\n        alternative_match = alternative_pattern.search(easyocr_text)\n        if alternative_match:\n            easyocr_answer = alternative_match.group(1)\n            answer_dict[str_i][\"EasyOCR\"] = easyocr_answer\n            answer_dict[str_i][\"FinalAnswer\"] = easyocr_answer\n            print(f\"Question {i}: Found answer {easyocr_answer} in EasyOCR recheck\")\n\n    combined_results = {int(number): answer_dict[number][\"FinalAnswer\"] for number in sorted(answer_dict, key=int) if answer_dict[number][\"FinalAnswer\"]}\n    conflict_results = {int(number): {\"Tesseract\": answer_dict[number][\"Tesseract\"], \"EasyOCR\": answer_dict[number][\"EasyOCR\"]} for number in conflicting_questions}\n\n    output_data = {\n        \"CombinedResults\": combined_results,\n        \"ConflictingQuestions\": conflict_results\n    }\n\n    output_filename = f'ocr_results_{os.path.basename(image_path).split(\".\")[0]}.json'\n    with open(output_filename, 'w') as json_file:\n        json.dump(output_data, json_file, indent=4)\n    print(f\"Results saved to '{output_filename}'\\n\")\n\n    print(\"Final Combined Results:\")\n    for number, answer in sorted(combined_results.items()):\n        print(f\"Question {number}: Answer {answer}\")\n\n    print(\"\\nJSON Content:\")\n    print(json.dumps(output_data, indent=4))\n\n    print(\"\\nConflicting Answers:\")\n    print(json.dumps(conflict_results, indent=4))\n\n# Set the directory containing images\nimage_directory = '/kaggle/input/qc174032-ans'\n\n# Collect all image file paths in the directory\nimage_paths = sorted([os.path.join(image_directory, file) for file in os.listdir(image_directory) if file.lower().endswith(('png', 'jpg', 'jpeg'))])\n\n# Process each image in the directory\nfor image_path in image_paths:\n    compare_ocr_methods(image_path)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Testing on KM24SF1 Answer","metadata":{}},{"cell_type":"code","source":"import cv2\nimport pytesseract\nimport re\nimport easyocr\nimport numpy as np\nimport json\nfrom collections import defaultdict\nimport os\n\ndef preprocess_image(image):\n    \"\"\" Preprocess the image to enhance OCR accuracy. \"\"\"\n    image = cv2.resize(image, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n    sharpened = cv2.filter2D(blurred, -1, kernel)\n    _, thresh = cv2.threshold(sharpened, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n    return thresh\n\ndef tesseract_extract_text(image):\n    \"\"\" Extract text using Tesseract OCR. \"\"\"\n    data = pytesseract.image_to_data(image, config='--psm 6', output_type=pytesseract.Output.DICT)\n    extracted_text = \" \".join([data['text'][i] for i in range(len(data['text'])) if int(data['conf'][i]) > 0])\n    return extracted_text\n\ndef easyocr_extract_text(image):\n    \"\"\" Extract text using EasyOCR. \"\"\"\n    reader = easyocr.Reader(['en', 'ms'])\n    results = reader.readtext(image, detail=0, contrast_ths=0.7, text_threshold=0.6, low_text=0.5)\n    return ' '.join(results)\n\ndef extract_answers(text):\n    \"\"\"\n    Extract answers where a number is followed by a valid answer (A, B, C, or D).\n    Ignores invalid entries.\n    \"\"\"\n    answer_pattern = re.compile(r'(\\d+)\\s+([A-D])\\b')  # Matches 'number' followed by a valid answer A-D\n    return answer_pattern.findall(text)\n\ndef detect_missing_questions(final_answers):\n    \"\"\" Detect and add missing questions with null values. \"\"\"\n    all_numbers = set(range(1, max(map(int, final_answers.keys())) + 1))\n    existing_numbers = set(map(int, final_answers.keys()))\n    missing_numbers = sorted(all_numbers - existing_numbers)\n\n    for num in missing_numbers:\n        final_answers[str(num)] = None\n    return dict(sorted(final_answers.items(), key=lambda x: int(x[0])))\n\ndef process_chapter_images(image_paths, chapter):\n    \"\"\" Process images for a single chapter. \"\"\"\n    chapter_results = {}\n    final_answers = {}\n    conflicting_questions = {}\n\n    for image_path in image_paths:\n        image = cv2.imread(image_path)\n        preprocessed_image = preprocess_image(image)\n\n        # Extract text with both OCR methods\n        tesseract_text = tesseract_extract_text(preprocessed_image)\n        easyocr_text = easyocr_extract_text(preprocessed_image)\n\n        # Extract answers\n        tesseract_answers = extract_answers(tesseract_text)\n        easyocr_answers = extract_answers(easyocr_text)\n\n        # Combine and resolve answers\n        combined_answers = defaultdict(lambda: {\"Tesseract\": None, \"EasyOCR\": None, \"Final\": None})\n\n        # Store answers from Tesseract\n        for num, ans in tesseract_answers:\n            combined_answers[num][\"Tesseract\"] = ans\n\n        # Store answers from EasyOCR and resolve conflicts\n        for num, ans in easyocr_answers:\n            combined_answers[num][\"EasyOCR\"] = ans\n            if combined_answers[num][\"Tesseract\"]:\n                if combined_answers[num][\"Tesseract\"] == ans:\n                    combined_answers[num][\"Final\"] = ans  # Both agree\n                else:\n                    combined_answers[num][\"Final\"] = ans  # Prioritize EasyOCR\n                    conflicting_questions[num] = {\n                        \"Tesseract\": combined_answers[num][\"Tesseract\"],\n                        \"EasyOCR\": ans\n                    }\n            else:\n                combined_answers[num][\"Final\"] = ans  # Tesseract is empty, use EasyOCR\n\n        # Update logic to use Tesseract if EasyOCR is None\n        for num, answer_data in combined_answers.items():\n            if answer_data[\"EasyOCR\"] is None and answer_data[\"Tesseract\"] is not None:\n                combined_answers[num][\"Final\"] = answer_data[\"Tesseract\"]  # Use Tesseract if EasyOCR is None\n\n        # Consolidate answers into final format\n        for num, answer_data in combined_answers.items():\n            final_answers[num] = answer_data[\"Final\"]\n\n        # Store intermediate results\n        chapter_results[os.path.basename(image_path)] = {\n            \"TesseractText\": tesseract_text,\n            \"EasyOCRText\": easyocr_text,\n            \"Answers\": combined_answers\n        }\n\n    # Detect missing questions and fill with \"Missing\"\n    final_answers = detect_missing_questions(final_answers)\n    return chapter_results, final_answers, conflicting_questions\n\n\n\ndef save_combined_results(final_answers, conflicting_questions, output_directory, chapter):\n    \"\"\" Save combined results in proper numerical sequence for each chapter. \"\"\"\n    output_data = {\n        \"CombinedResults\": final_answers,\n        \"ConflictingQuestions\": conflicting_questions\n    }\n    output_file = os.path.join(output_directory, f\"chapter_C{chapter}_final.json\")\n    with open(output_file, 'w') as json_file:\n        json.dump(output_data, json_file, indent=4)\n    print(f\"Final combined results saved to {output_file}\")\n\ndef main():\n    image_directory = '/kaggle/input/km24sf1-ans'  # Set to your folder path\n    output_directory = '/kaggle/working/km24sf1-ocr-ans-final'\n    os.makedirs(output_directory, exist_ok=True)\n    \n    # Group files by chapter (C1, C2, C3)\n    chapter_groups = defaultdict(list)\n    for file in os.listdir(image_directory):\n        if file.endswith(('.png', '.jpg', '.jpeg')):\n            chapter = re.search(r'_C(\\d+)_', file).group(1)  # Extract chapter (C1, C2, C3)\n            chapter_groups[chapter].append(os.path.join(image_directory, file))\n    \n    # Process each chapter\n    all_results = {}\n\n    for chapter, files in chapter_groups.items():\n        print(f\"\\n{'=' * 20} Processing Chapter C{chapter} ({len(files)} files) {'=' * 20}\\n\")\n        chapter_results, final_answers, conflicting_questions = process_chapter_images(sorted(files), chapter)\n\n        # Print results for each file\n        for file_name, result in chapter_results.items():\n            print(f\"\\nFile: {file_name}\")\n            print(f\"Tesseract Text: {result['TesseractText']}\")\n            print(f\"EasyOCR Text: {result['EasyOCRText']}\")\n            print(\"Extracted Answers:\")\n            for qnum, answers in result['Answers'].items():\n                print(f\"  Q{qnum}: Tesseract={answers['Tesseract']}, EasyOCR={answers['EasyOCR']}, Final={answers['Final']}\")\n\n        # Save all chapter results\n        all_results[f\"Chapter_C{chapter}\"] = chapter_results\n\n        # Save detailed chapter results as JSON\n        output_file = os.path.join(output_directory, f\"chapter_C{chapter}_results.json\")\n        with open(output_file, 'w') as json_file:\n            json.dump(chapter_results, json_file, indent=4)\n        print(f\"\\nSaved detailed results for Chapter C{chapter} to {output_file}\")\n\n        # Save final consolidated results for the chapter\n        save_combined_results(final_answers, conflicting_questions, output_directory, chapter)\n\n        # Save final consolidated answers for this chapter into an individual JSON file\n        final_answers_file = os.path.join(output_directory, f\"chapter_C{chapter}_final_answers.json\")\n        with open(final_answers_file, 'w') as json_file:\n            json.dump(final_answers, json_file, indent=4)\n        print(f\"Saved final consolidated answers for Chapter C{chapter} to {final_answers_file}\")\n\n        # Print sorted consolidated answers\n        print(f\"\\nFinal Consolidated Answers for Chapter C{chapter}:\")\n        for qnum, answer in sorted(final_answers.items(), key=lambda x: int(x[0])):\n            print(f\"  Question {qnum}: {answer if answer else 'Missing'}\")\n        print(\"=\" * 60)\n\n    # Save all results combined into a single file\n    all_results_file = os.path.join(output_directory, 'all_chapters_results.json')\n    with open(all_results_file, 'w') as json_file:\n        json.dump(all_results, json_file, indent=4)\n    print(f\"\\nAll detailed results saved to {all_results_file}\")\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T09:43:54.089075Z","iopub.execute_input":"2024-12-16T09:43:54.089480Z","iopub.status.idle":"2024-12-16T09:44:51.172938Z","shell.execute_reply.started":"2024-12-16T09:43:54.089446Z","shell.execute_reply":"2024-12-16T09:44:51.172086Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Testing on Answer GG24SF14_ans column cropped","metadata":{}},{"cell_type":"code","source":"# This script performs OCR using Tesseract and EasyOCR on an input image containing multiple-choice answers.\n# Currently, the input images must be structured in a single column format for proper extraction.\n# Only images with a similar format to 'Picture 2' are supported, as 'Picture 1' includes two separate columns that make the OCR challenging.\n# The script extracts answers from the images, compares the results from both OCR methods, and resolves conflicting answers by using the EasyOCR result.\n\n\nimport cv2\nimport pytesseract\nimport re\nimport easyocr\nimport numpy as np\nimport json\nfrom collections import defaultdict\n\ndef preprocess_image(image):\n    # Preprocess the image to enhance OCR accuracy: resizing, grayscale conversion, blurring, sharpening, and thresholding.\n    image = cv2.resize(image, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n    sharpened = cv2.filter2D(blurred, -1, kernel)\n    _, thresh = cv2.threshold(sharpened, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n    return thresh\n\ndef tesseract_extract_text(image):\n    # Extract text using Tesseract OCR.\n    data = pytesseract.image_to_data(image, config='--psm 6', output_type=pytesseract.Output.DICT)\n    extracted_text = \" \".join([data['text'][i] for i in range(len(data['text'])) if int(data['conf'][i]) > 0])\n    return extracted_text\n\ndef easyocr_extract_text(image):\n    # Extract text using EasyOCR.\n    reader = easyocr.Reader(['en', 'ms'])\n    results = reader.readtext(image, detail=0, contrast_ths=0.7, text_threshold=0.6, low_text=0.5)\n    return ' '.join(results)\n\ndef extract_answers(text):\n    # Match answers with or without a period\n    answer_pattern = re.compile(r'(\\d+)\\s*\\.?\\s*([A-D])')\n    return answer_pattern.findall(text)\n\n\ndef compare_ocr_methods(image_path):\n    # Compare the results of Tesseract and EasyOCR, and resolve conflicts by using EasyOCR answers when discrepancies are found.\n    image = cv2.imread(image_path)\n    preprocessed_image = preprocess_image(image)\n\n    tesseract_text = tesseract_extract_text(preprocessed_image)\n    tesseract_answers = extract_answers(tesseract_text)\n\n    easyocr_text = easyocr_extract_text(preprocessed_image)\n    easyocr_answers = extract_answers(easyocr_text)\n\n    answer_dict = defaultdict(lambda: {\"Tesseract\": None, \"EasyOCR\": None})\n    for number, answer in tesseract_answers:\n        answer_dict[number][\"Tesseract\"] = answer\n\n    for number, answer in easyocr_answers:\n        if answer_dict[number][\"Tesseract\"] is None:\n            answer_dict[number][\"EasyOCR\"] = answer\n        elif answer_dict[number][\"Tesseract\"] != answer:\n            answer_dict[number][\"EasyOCR\"] = answer\n\n    print(f\"Results for {image_path}:\")\n    print(f\"Tesseract OCR Results: {tesseract_text}\")\n    print(f\"EasyOCR Results: {easyocr_text}\")\n    print(\"Combined Results (Sorted):\")\n\n    missing_questions = []\n    conflicting_questions = []\n\n    max_question = max(map(int, answer_dict.keys())) if answer_dict else 0\n    for i in range(1, max_question + 1):\n        str_i = str(i)\n        if str_i not in answer_dict:\n            missing_questions.append(i)\n        else:\n            tesseract_answer = answer_dict[str_i][\"Tesseract\"]\n            easyocr_answer = answer_dict[str_i][\"EasyOCR\"]\n\n            if tesseract_answer and easyocr_answer and tesseract_answer != easyocr_answer:\n                conflicting_questions.append(i)\n                print(f\"Question {i}: Conflicting Answers Detected: Tesseract: {tesseract_answer}, EasyOCR: {easyocr_answer} (Using EasyOCR)\")\n                answer_dict[str_i][\"FinalAnswer\"] = easyocr_answer\n            elif tesseract_answer:\n                print(f\"Question {i}: Answer {tesseract_answer} (Detected by Tesseract)\")\n                answer_dict[str_i][\"FinalAnswer\"] = tesseract_answer\n            elif easyocr_answer:\n                print(f\"Question {i}: Answer {easyocr_answer} (Detected by EasyOCR)\")\n                answer_dict[str_i][\"FinalAnswer\"] = easyocr_answer\n\n    if missing_questions:\n        print(f\"\\nMissing Questions: {', '.join(map(str, missing_questions))}\")\n    if conflicting_questions:\n        print(f\"\\nConflicting Questions: {', '.join(map(str, conflicting_questions))} (Needs Recheck)\")\n    print(\"\\n\" + \"=\"*50 + \"\\n\")\n\n    for i in missing_questions:\n        print(f\"Rechecking missing question {i}...\")\n        str_i = str(i)\n\n        alternative_pattern = re.compile(fr'{i}\\s*([A-D])')\n        alternative_match = alternative_pattern.search(tesseract_text)\n        if alternative_match:\n            tesseract_answer = alternative_match.group(1)\n            answer_dict[str_i][\"Tesseract\"] = tesseract_answer\n            answer_dict[str_i][\"FinalAnswer\"] = tesseract_answer\n            print(f\"Question {i}: Found answer {tesseract_answer} in Tesseract recheck\")\n            continue\n\n        alternative_match = alternative_pattern.search(easyocr_text)\n        if alternative_match:\n            easyocr_answer = alternative_match.group(1)\n            answer_dict[str_i][\"EasyOCR\"] = easyocr_answer\n            answer_dict[str_i][\"FinalAnswer\"] = easyocr_answer\n            print(f\"Question {i}: Found answer {easyocr_answer} in EasyOCR recheck\")\n\n    combined_results = {\n        int(number): answer_dict[number].get(\"FinalAnswer\", None)\n        for number in sorted(answer_dict, key=int)\n        if answer_dict[number].get(\"FinalAnswer\") and 1 <= int(number) <= 50  # Valid range filter\n    }\n    \n    conflict_results = {\n        int(number): {\"Tesseract\": answer_dict[number][\"Tesseract\"], \"EasyOCR\": answer_dict[number][\"EasyOCR\"]}\n        for number in conflicting_questions\n    }\n    \n    output_data = {\n        \"CombinedResults\": combined_results,\n        \"ConflictingQuestions\": conflict_results\n    }\n    \n    output_filename = f'ocr_results_{image_path.split(\"/\")[-1].split(\".\")[0]}.json'\n    with open(output_filename, 'w') as json_file:\n        json.dump(output_data, json_file, indent=4)\n    print(f\"Results saved to '{output_filename}'\\n\")\n\n\n    print(\"Final Combined Results:\")\n    for number, answer in sorted(combined_results.items()):\n        print(f\"Question {number}: Answer {answer}\")\n\n    print(\"\\nJSON Content:\")\n    print(json.dumps(output_data, indent=4))\n\n    print(\"\\nConflicting Answers:\")\n    print(json.dumps(conflict_results, indent=4))\n\nimage_paths = [\n    \"/kaggle/input/qc174032-ans/Ujian_1.png\",\n    \"/kaggle/input/qc174032-ans/Ujian_2.png\",\n    \"/kaggle/input/qc174032-ans/Ujian_3.png\",\n    \"/kaggle/input/qc174032-ans/Ujian_4.png\",\n    \"/kaggle/input/qc174032-ans/Ujian_5.png\",\n    \"/kaggle/input/qc174032-ans/Ujian_6.png\",\n    \"/kaggle/input/qc174032-ans/Ujian_7.png\",\n    \"/kaggle/input/qc174032-ans/Ujian_8.png\",\n    \"/kaggle/input/qc174032-ans/Ujian_9.png\",\n    \"/kaggle/input/qc174032-ans/Ujian_10.png\",\n    \"/kaggle/input/qc174032-ans/Ujian_11.png\",\n    \"/kaggle/input/qc174032-ans/Ujian_12.png\",\n    \"/kaggle/input/qc174032-ans/Ujian_13.png\",\n    \"/kaggle/input/qc174032-ans/Ujian_14.png\"    \n]\n\nfor image_path in image_paths:\n    compare_ocr_methods(image_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T13:10:10.662601Z","iopub.execute_input":"2024-12-18T13:10:10.662944Z","iopub.status.idle":"2024-12-18T13:11:10.580393Z","shell.execute_reply.started":"2024-12-18T13:10:10.662912Z","shell.execute_reply":"2024-12-18T13:11:10.579443Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Comparing the answer and the question file name","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\nimport re\n\n# Define the source and destination directories\nsource_directory = \"/kaggle/input/qc174032-ans-latest-231224\"\ndestination_directory = \"/kaggle/working/qc174032-ans-latest-renamed_1\"\n\n# Create the destination directory if it doesn't exist\nos.makedirs(destination_directory, exist_ok=True)\n\n# Regular expression to match and extract the chapter number\npattern = r\"ocr_results_QC174032_(\\d+)\\.json\"\n\n# Copy and rename the files\nfor file_name in os.listdir(source_directory):\n    match = re.match(pattern, file_name)\n    if match:\n        # Extract the chapter number using the regex\n        chapter = match.group(1)  # Extract the numeric part (e.g., '12')\n\n        # Create the new file name\n        new_file_name = f\"ocr_results_QC174032_C{chapter}.json\"  # Add 'C' before the chapter number\n\n        # Define old and new paths\n        old_path = os.path.join(source_directory, file_name)\n        new_path = os.path.join(destination_directory, new_file_name)\n\n        # Copy the file to the destination directory with the new name\n        shutil.copy2(old_path, new_path)\n        print(f\"Copied and renamed: {file_name} -> {new_file_name}\")\n\nprint(\"Renaming and copying complete!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T07:50:25.519638Z","iopub.execute_input":"2024-12-23T07:50:25.519983Z","iopub.status.idle":"2024-12-23T07:50:25.544082Z","shell.execute_reply.started":"2024-12-23T07:50:25.519954Z","shell.execute_reply":"2024-12-23T07:50:25.543219Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Rename Ans file name","metadata":{}},{"cell_type":"code","source":"import os\nimport re\n\n# Directories for the response and result files\nresponse_dir = \"/kaggle/input/qc174032-question-latest231224\"\nresult_dir = \"/kaggle/working/QC174032_QA_en_ms_latest/english\"\n\ndef extract_chapter_question(file_name, pattern):\n    \"\"\"Extract chapter and question from a file name based on a pattern.\"\"\"\n    match = re.search(pattern, file_name)\n    if match:\n        return match.group(1), match.group(2)  # Return chapter and question as strings\n    return None, None\n\ndef get_chapters_questions(directory, pattern):\n    \"\"\"Get chapters and questions from files in a directory.\"\"\"\n    chapters_questions = set()\n    unmatched_files = []  # To track unmatched files\n    for file_name in os.listdir(directory):\n        chapter, question = extract_chapter_question(file_name, pattern)\n        if chapter and question:\n            chapters_questions.add((chapter, question))\n        else:\n            unmatched_files.append(file_name)  # Track unmatched files\n    # Log unmatched files\n    if unmatched_files:\n        print(f\"Unmatched files in {directory}:\")\n        print(unmatched_files)\n    return chapters_questions\n\ndef compare_files(response_dir, result_dir):\n    \"\"\"Compare response and result files and identify missing entries.\"\"\"\n    response_pattern = r\"C(\\d+)_Q(\\d+)_response\\.json\"\n    result_pattern = r\"C(\\d+)_Q(\\d+)_en\\.json\"\n\n    response_chapters_questions = get_chapters_questions(response_dir, response_pattern)\n    result_chapters_questions = get_chapters_questions(result_dir, result_pattern)\n\n    # Find missing in results\n    missing_in_results = response_chapters_questions - result_chapters_questions\n\n    # Find extra in results\n    extra_in_results = result_chapters_questions - response_chapters_questions\n\n    return missing_in_results, extra_in_results, len(response_chapters_questions), len(result_chapters_questions)\n\n# Compare the files\nmissing_in_results, extra_in_results, total_response, total_results = compare_files(response_dir, result_dir)\n\n# Print results\nprint(f\"Total files in response: {total_response}\")\nprint(f\"Total files in results: {total_results}\")\nprint(\"Missing in results:\")\nfor chapter, question in sorted(missing_in_results):\n    print(f\"Chapter: {chapter}, Question: {question}\")\n\nprint(\"\\nExtra in results:\")\nfor chapter, question in sorted(extra_in_results):\n    print(f\"Chapter: {chapter}, Question: {question}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T07:53:35.837185Z","iopub.execute_input":"2024-12-23T07:53:35.837816Z","iopub.status.idle":"2024-12-23T07:53:35.850286Z","shell.execute_reply.started":"2024-12-23T07:53:35.837781Z","shell.execute_reply":"2024-12-23T07:53:35.849316Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Workflow for Combining Responses and Answers to dataset\n\nThis workflow explains how to process and combine questions (responses) and their answers from the provided directories into structured outputs.\n\n---\n\n### **Step 1: Set Directories**\n- Define directories for:\n  - **Responses**: Contains question files (e.g., `C1_Q1_response.json`).\n  - **Answers**: Contains answer files (e.g., `C1_answers.json`).\n\n---\n\n### **Step 2: Load Metadata**\n- Load **chapter metadata** for each book to add information like:\n  - Book name, publisher, ISBN.\n  - Chapter topics.\n\n---\n\n### **Step 3: Match Files**\n- For each response file:\n  1. Find the corresponding answer file using naming patterns.\n  2. Extract the question details and match them with the answer.\n\n---\n\n### **Step 4: Enrich Data**\n- Combine the question and answer details.\n- Add metadata such as:\n  - Book details.\n  - Chapter topics.\n  - Question text, options, and figures.\n\n---\n\n### **Step 5: Save Outputs**\n- Save the combined data into two formats:\n  - **English**: `C1_Q1_en.json`\n  - **Malay**: `C1_Q1_ms.json`\n- Organize outputs into:\n  - `/english`\n  - `/malay`\n\n---\n\n### **Step 6: Handle Missing Data**\n- Log questions that couldn't be processed due to:\n  - Missing answer files.\n  - Empty or unmatched answers.\n\n---\n\n### Example:\n1. Input:\n   - Responses: `/qc174032-response`\n   - Answers: `/qc174032-answers`\n2. Output:\n   - `/english/qc174032_C1_Q1_en.json`\n   - `/malay/qc174032_C1_Q1_ms.json`\n\n","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport re\nfrom pathlib import Path\nimport glob\n\n# Define the pairs of response and answer directories\ndata_pairs = [\n    {\n        \"responses_dir\": \"/kaggle/input/fc064244-cleaned\",\n        \"answers_dir\": \"/kaggle/input/fc064244-ans-latest-1\"\n    },\n    {\n        \"responses_dir\": \"/kaggle/input/fc064244-response-2\",\n        \"answers_dir\": \"/kaggle/input/fc064244-ans-latest-1\"\n    },\n    {\n        \"responses_dir\": \"/kaggle/input/fc065244-response\",\n        \"answers_dir\": \"/kaggle/input/fc065244-ans-latest-1\"\n    },\n    {\n        \"responses_dir\": \"/kaggle/input/qc174032-question-latest231224\",\n        \"answers_dir\": \"/kaggle/input/qc174032-ans-latest1-23-12-14\"\n    },\n    {\n        \"responses_dir\": \"/kaggle/input/km24sf1-response\",\n        \"answers_dir\": \"/kaggle/input/km24sf1-ans-json\"\n    },\n    {\n        \"responses_dir\": \"/kaggle/input/km24sma-response\",\n        \"answers_dir\": \"/kaggle/input/km24sma-ans-latest\"\n    },\n    {\n        \"responses_dir\": \"/kaggle/input/gg24sf14-response\",\n        \"answers_dir\": \"/kaggle/input/gg24sf14-ans-latest-1\"\n    }\n]\n\n# Define chapters directory\nchapters_dir = \"/kaggle/input/book-info\"\n\ndef load_chapters(book_name):\n    \"\"\"Load chapters JSON for a specific book.\"\"\"\n    chapter_file = os.path.join(chapters_dir, f\"{book_name}.json\")\n    if os.path.exists(chapter_file):\n        with open(chapter_file, \"r\", encoding=\"utf-8\") as f:\n            return json.load(f)\n    return {}\n\ndef save_json(data, directory, filename):\n    \"\"\"Helper function to save JSON files.\"\"\"\n    path = os.path.join(directory, filename)\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=4, ensure_ascii=False)\n    print(f\"Saved: {path}\")\n    print(json.dumps(data, indent=4, ensure_ascii=False))  # Print the JSON content\n\ndef get_topic_from_chapters(chapters, chapter_label):\n    \"\"\"Fetch the topic from chapters based on the chapter label.\"\"\"\n    for chapter in chapters.get(\"chapters\", []):\n        if chapter.get(\"label\", \"\") == chapter_label:\n            return chapter.get(\"topic\", {})\n    return {}\n\ndef prepare_english_output(book_name, chapters, chapter, question, response, answer):\n    topic = get_topic_from_chapters(chapters, f\"C{chapter}\")\n    return {\n        \"book_no\": book_name,\n        \"book_name\": chapters.get(\"book_name\", \"\"),\n        \"publisher\": chapters.get(\"publisher\", \"\"),\n        \"IBSN\": chapters.get(\"IBSN\", \"\"),\n        \"subject\": chapters.get(\"subject\", {}).get(\"english\", \"\"),\n        \"topic\": topic.get(\"english\", \"\"),\n        \"text\": response.get(\"text\", {}).get(\"english\", \"\"),\n        \"figures\": [\n            {\n                \"label\": figure.get(\"label\", {}).get(\"english\", \"\"),\n                \"path\": figure.get(\"path\", \"\")\n            }\n            for figure in response.get(\"figures\", [])\n        ],\n        \"options\": {k: v.get(\"english\", \"\") for k, v in response.get(\"options\", {}).items()},\n        \"answers\": answer\n    }\n\ndef prepare_malay_output(book_name, chapters, chapter, question, response, answer):\n    topic = get_topic_from_chapters(chapters, f\"C{chapter}\")\n    return {\n        \"book_no\": book_name,\n        \"book_name\": chapters.get(\"book_name\", \"\"),\n        \"publisher\": chapters.get(\"publisher\", \"\"),\n        \"IBSN\": chapters.get(\"IBSN\", \"\"),\n        \"subject\": chapters.get(\"subject\", {}).get(\"malay\", \"\"),\n        \"topic\": topic.get(\"malay\", \"\"),\n        \"text\": response.get(\"text\", {}).get(\"malay\", \"\"),\n        \"figures\": [\n            {\n                \"label\": figure.get(\"label\", {}).get(\"malay\", \"\"),\n                \"path\": figure.get(\"path\", \"\")\n            }\n            for figure in response.get(\"figures\", [])\n        ],\n        \"options\": {k: v.get(\"malay\", \"\") for k, v in response.get(\"options\", {}).items()},\n        \"answers\": answer\n    }\n\ndef find_answer_file(book_name, chapter, answers_dir):\n    \"\"\"Find the correct answer file dynamically based on various naming patterns.\"\"\"\n    patterns = [\n        os.path.join(answers_dir, f\"ocr_results_{book_name}_C{chapter}_ANS.json\"),\n        os.path.join(answers_dir, f\"ocr_results_{book_name}_C{chapter}_ans.json\"),\n        os.path.join(answers_dir, f\"ocr_results_{book_name}_C{chapter}.json\"),\n        os.path.join(answers_dir, f\"ocr_results_{book_name}_*C{chapter}*.json\"),\n        os.path.join(answers_dir, f\"{book_name}_C{chapter}_ans.json\"),\n        os.path.join(answers_dir, f\"{book_name}_C{chapter}*.json\"),\n        os.path.join(answers_dir, f\"*{chapter}*.json\"),  # Broad wildcard for unmatched cases\n    ]\n    \n    for pattern in patterns:\n        files = glob.glob(pattern)\n        if files:\n            return files[0]\n    \n    print(f\"No matching answer file found for book: {book_name}, chapter: {chapter}\")\n    return None\n\ndef main():\n    print(\"Processing responses and answers...\")\n\n    for pair in data_pairs:\n        responses_dir = pair[\"responses_dir\"]\n        answers_dir = pair[\"answers_dir\"]\n\n        response_files = [f for f in os.listdir(responses_dir) if f.endswith(\"_response.json\")]\n        book_name = re.match(r\"([A-Z0-9]+)_\", response_files[0]).group(1)\n        chapters = load_chapters(book_name)\n\n        # Special condition for KM24SF1 to rename the output folder and file names to KM24SFI\n        if book_name == \"KM24SF1\":\n            book_name_to_save = \"KM24SFI\"\n        else:\n            book_name_to_save = book_name\n\n        output_dir_english = os.path.join(f\"./{book_name_to_save}_QA_en_ms_latest\", \"english\")\n        output_dir_malay = os.path.join(f\"./{book_name_to_save}_QA_en_ms_latest\", \"malay\")\n        os.makedirs(output_dir_english, exist_ok=True)\n        os.makedirs(output_dir_malay, exist_ok=True)\n\n        for file in response_files:\n            match = re.match(rf\"{book_name}_C(\\d+)_Q(\\d+)_response.json\", file)\n            if match:\n                chapter = match.group(1)\n                question = match.group(2)\n                response_path = os.path.join(responses_dir, file)\n\n                # Dynamically find the answer file\n                answer_path = find_answer_file(book_name, chapter, answers_dir)\n                if not answer_path:\n                    print(f\"No answer file for {file}\")\n                    continue\n\n                with open(response_path, \"r\", encoding=\"utf-8\") as f:\n                    response = json.load(f)\n                with open(answer_path, \"r\", encoding=\"utf-8\") as f:\n                    answers = json.load(f)\n\n                # Check for CombinedResults or directly search for the question key\n                if \"CombinedResults\" in answers:\n                    answer = answers.get(\"CombinedResults\", {}).get(question, \"\")\n                else:\n                    # Fallback: Directly access the question as a key in the top-level dictionary\n                    answer = answers.get(question, \"\")\n\n                # Skip if no answer is found\n                if not answer:\n                    print(f\"No answer found for question {question} in {answer_path}.\")\n                    continue\n\n                english_output = prepare_english_output(book_name, chapters, chapter, question, response, answer)\n                malay_output = prepare_malay_output(book_name, chapters, chapter, question, response, answer)\n\n                save_json(english_output, output_dir_english, f\"{book_name_to_save}_C{chapter}_Q{question}_en.json\")\n                save_json(malay_output, output_dir_malay, f\"{book_name_to_save}_C{chapter}_Q{question}_ms.json\")\n\n    print(\"Processing complete!\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Testing for 1 response and 1 ans","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport re\nfrom pathlib import Path\n\n# Define file directories\nresponses_dir = \"/kaggle/input/km24sf1-response\"\nanswers_dir = \"/kaggle/input/km24sf1-ans-json\"\n\nchapters_dir = \"/kaggle/input/book-info\"  \n\noutput_dir_english = \"./processed_outputs/english\"\noutput_dir_malay = \"./processed_outputs/malay\"\nos.makedirs(output_dir_english, exist_ok=True)\nos.makedirs(output_dir_malay, exist_ok=True)\n\n\ndef load_chapters(book_name):\n    \"\"\"Load chapters JSON for a specific book.\"\"\"\n    chapter_file = os.path.join(chapters_dir, f\"{book_name}.json\")\n    if os.path.exists(chapter_file):\n        with open(chapter_file, \"r\") as f:\n            return json.load(f)\n    return {}\n\ndef save_json(data, directory, filename):\n    \"\"\"Helper function to save JSON files.\"\"\"\n    path = os.path.join(directory, filename)\n    with open(path, \"w\") as f:\n        json.dump(data, f, indent=4)\n    print(f\"Saved: {path}\")\n    print(json.dumps(data, indent=4))  # Print the JSON content\n\ndef get_topic_from_chapters(chapters, chapter_label):\n    \"\"\"Fetch the topic from chapters based on the chapter label.\"\"\"\n    for chapter in chapters.get(\"chapters\", []):\n        if chapter.get(\"label\", \"\") == chapter_label:\n            return chapter.get(\"topic\", {})\n    return {}\n\ndef prepare_english_output(book_name, chapters, chapter, question, response, answer):\n    topic = get_topic_from_chapters(chapters, f\"C{chapter}\")\n    return {\n        \"book_no\": book_name,\n        \"book_name\": chapters.get(\"book_name\", \"\"),\n        \"publisher\": chapters.get(\"publisher\", \"\"),\n        \"IBSN\": chapters.get(\"IBSN\", \"\"),\n        \"subject\": chapters.get(\"subject\", {}).get(\"english\", \"\"),\n        \"topic\": topic.get(\"english\", \"\"),\n        \"text\": response.get(\"text\", {}).get(\"english\", \"\"),\n        \"figures\": [\n            {\n                \"label\": figure.get(\"label\", {}).get(\"english\", \"\"),\n                \"path\": figure.get(\"path\", \"\")\n            }\n            for figure in response.get(\"figures\", [])\n        ],\n        \"options\": {k: v.get(\"english\", \"\") for k, v in response.get(\"options\", {}).items()},\n        \"answers\": answer\n    }\n\ndef prepare_malay_output(book_name, chapters, chapter, question, response, answer):\n    topic = get_topic_from_chapters(chapters, f\"C{chapter}\")\n    return {\n        \"book_no\": book_name,\n        \"book_name\": chapters.get(\"book_name\", \"\"),\n        \"publisher\": chapters.get(\"publisher\", \"\"),\n        \"IBSN\": chapters.get(\"IBSN\", \"\"),\n        \"subject\": chapters.get(\"subject\", {}).get(\"malay\", \"\"),\n        \"topic\": topic.get(\"malay\", \"\"),\n        \"text\": response.get(\"text\", {}).get(\"malay\", \"\"),\n        \"figures\": [\n            {\n                \"label\": figure.get(\"label\", {}).get(\"malay\", \"\"),\n                \"path\": figure.get(\"path\", \"\")\n            }\n            for figure in response.get(\"figures\", [])\n        ],\n        \"options\": {k: v.get(\"malay\", \"\") for k, v in response.get(\"options\", {}).items()},\n        \"answers\": answer\n    }\n\nimport glob\n\ndef find_answer_file(book_name, chapter):\n    \"\"\"Find the correct answer file dynamically based on various naming patterns.\"\"\"\n    patterns = [\n        os.path.join(answers_dir, f\"ocr_results_{book_name}_C{chapter}_ANS.json\"),\n        os.path.join(answers_dir, f\"ocr_results_{book_name}_C{chapter}_ans.json\"),\n        os.path.join(answers_dir, f\"ocr_results_{book_name}_C{chapter}.json\"),\n        os.path.join(answers_dir, f\"ocr_results_{book_name}_*C{chapter}*.json\"),\n        os.path.join(answers_dir, f\"{book_name}_C{chapter}_ans.json\"),\n        os.path.join(answers_dir, f\"{book_name}_C{chapter}*.json\"),\n        os.path.join(answers_dir, f\"*{chapter}*.json\"),  # Broad wildcard for unmatched cases\n    ]\n    \n    for pattern in patterns:\n        #print(f\"Checking pattern: {pattern}\")  # Debugging print\n        files = glob.glob(pattern)\n        if files:\n            #print(f\"Matched file: {files[0]}\")  # Debugging print\n            return files[0]\n    \n    # If no matching file is found\n    print(f\"No matching answer file found for book: {book_name}, chapter: {chapter}\")\n    return None\n\n\n\ndef main():\n    print(\"Processing responses and answers...\")\n    response_files = [f for f in os.listdir(responses_dir) if f.endswith(\"_response.json\")]\n    book_name = re.match(r\"([A-Z0-9]+)_\", response_files[0]).group(1)\n    chapters = load_chapters(book_name)\n\n    for file in response_files:\n        match = re.match(rf\"{book_name}_C(\\d+)_Q(\\d+)_response.json\", file)\n        if match:\n            chapter = match.group(1)\n            question = match.group(2)\n            response_path = os.path.join(responses_dir, file)\n\n            # Dynamically find the answer file\n            answer_path = find_answer_file(book_name, chapter)\n            if not answer_path:\n                print(f\"No answer file for {file}\")\n                continue\n            \n            with open(response_path, \"r\") as f:\n                response = json.load(f)\n            with open(answer_path, \"r\") as f:\n                answers = json.load(f)\n\n            # Check for CombinedResults or directly search for the question key\n            if \"CombinedResults\" in answers:\n                answer = answers.get(\"CombinedResults\", {}).get(question, \"\")\n            else:\n                # Fallback: Directly access the question as a key in the top-level dictionary\n                answer = answers.get(question, \"\")\n\n            # Skip if no answer is found\n            if not answer:\n                print(f\"No answer found for question {question} in {answer_path}.\")\n                continue\n\n            english_output = prepare_english_output(book_name, chapters, chapter, question, response, answer)\n            malay_output = prepare_malay_output(book_name, chapters, chapter, question, response, answer)\n\n            save_json(english_output, output_dir_english, f\"{book_name}_C{chapter}_Q{question}_en.json\")\n            save_json(malay_output, output_dir_malay, f\"{book_name}_C{chapter}_Q{question}_ms.json\")\n\n    print(\"Processing complete!\")\n\n\n    \nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\n\n# Define the root directory and new output directories\nroot_dir = \"/kaggle/working\"\noutput_dir = \"/kaggle/working/combined_output\"\nenglish_output_dir = os.path.join(output_dir, \"english\")\nmalay_output_dir = os.path.join(output_dir, \"malay\")\n\n# Create the output directories if they do not exist\nos.makedirs(english_output_dir, exist_ok=True)\nos.makedirs(malay_output_dir, exist_ok=True)\n\n# Walk through the directory structure\nfor root, dirs, files in os.walk(root_dir):\n    if \"english\" in root:\n        # Copy files in the 'english' folder to the combined english_output_dir\n        for file in files:\n            src_file = os.path.join(root, file)\n            dest_file = os.path.join(english_output_dir, file)\n            # Skip if the file already exists at the destination\n            if not os.path.exists(dest_file):\n                shutil.copy(src_file, dest_file)\n    elif \"malay\" in root:\n        # Copy files in the 'malay' folder to the combined malay_output_dir\n        for file in files:\n            src_file = os.path.join(root, file)\n            dest_file = os.path.join(malay_output_dir, file)\n            # Skip if the file already exists at the destination\n            if not os.path.exists(dest_file):\n                shutil.copy(src_file, dest_file)\n\nprint(f\"Files combined successfully into {output_dir}.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T12:47:39.247671Z","iopub.execute_input":"2024-12-24T12:47:39.247987Z","iopub.status.idle":"2024-12-24T12:47:39.376697Z","shell.execute_reply.started":"2024-12-24T12:47:39.247963Z","shell.execute_reply":"2024-12-24T12:47:39.375935Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# Define the root directory\nroot_dir = \"/kaggle/working\"\n\n# Dictionary to store the counts\nfolder_file_counts = {}\n\n# Iterate through each folder in the root directory\nfor folder in os.listdir(root_dir):\n    folder_path = os.path.join(root_dir, folder)\n    if os.path.isdir(folder_path):  # Ensure it's a directory\n        folder_file_counts[folder] = {}\n        for subfolder in [\"english\", \"malay\"]:\n            subfolder_path = os.path.join(folder_path, subfolder)\n            if os.path.isdir(subfolder_path):  # Check if subfolder exists\n                # Count the number of files in the subfolder\n                file_count = len([file for file in os.listdir(subfolder_path) if os.path.isfile(os.path.join(subfolder_path, file))])\n                folder_file_counts[folder][subfolder] = file_count\n            else:\n                folder_file_counts[folder][subfolder] = 0  # No subfolder present\n\n# Print the results\nfor folder, subfolder_counts in folder_file_counts.items():\n    print(f\"Folder: {folder}\")\n    for subfolder, count in subfolder_counts.items():\n        print(f\"  {subfolder}: {count} file(s)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T12:47:43.479736Z","iopub.execute_input":"2024-12-24T12:47:43.480049Z","iopub.status.idle":"2024-12-24T12:47:43.503454Z","shell.execute_reply.started":"2024-12-24T12:47:43.480020Z","shell.execute_reply":"2024-12-24T12:47:43.502724Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Workflow for Exporting JSON to Table\n\nThis script processes JSON files containing question-and-answer data and exports them into a structured CSV table. Here's the simplified workflow:\n\n---\n\n### **Workflow**\n1. **Input Directory**:\n   - JSON files are stored in the directory `final_dataset_dir` (e.g., `/kaggle/input/final-dataset-ms`).\n   - Each JSON file represents a question, its options, answer, and related metadata.\n\n2. **Process Each JSON File**:\n   - For every file:\n     - Load the JSON data.\n     - Extract fields:\n       - **IBSN**: Unique identifier for the book.\n       - **Subject**: Subject of the question.\n       - **Topics**: Related chapter or topic.\n       - **Questions**: Main question text.\n       - **Figures**: Image details, if any (e.g., figure label and file path).\n       - **Options**: Answer choices (e.g., A, B, C, D).\n       - **Answers**: Correct answer for the question.\n     - Format the data into a row.\n\n3. **Build a Table**:\n   - Combine all rows into a structured **DataFrame** using Pandas.\n\n4. **Export as CSV**:\n   - Save the table to a CSV file (`final_dataset_table_ms.csv`) in the `output_table_dir` directory (e.g., `/kaggle/working/table`).\n\n---\n\n### **Customizable Parameters**\n- **Input Directory** (`final_dataset_dir`): Change to point to the directory containing the JSON files.\n- **Output Directory** (`output_table_dir`): Set the location to save the exported table.\n- **Fields to Extract**:\n  - You can add/remove fields to customize what is included in the table (e.g., add a new metadata field from the JSON).\n\n---\n\n### **Output**\n- A CSV file (`final_dataset_table_ms.csv`) containing:\n  - Columns: IBSN, Subject, Topics, Questions, Figures, Options, Answers.\n  - Rows: One row per JSON file/question.","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport pandas as pd\n\n# Define the directory containing the final JSON dataset\nfinal_dataset_dir = \"/kaggle/working/combined_output/malay\"\n\n# Initialize an empty list to store rows for the table\nrows = []\n\n# Iterate over all JSON files in the dataset directory\nfor file_name in os.listdir(final_dataset_dir):\n    if file_name.endswith(\".json\"):\n        file_path = os.path.join(final_dataset_dir, file_name)\n        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n            data = json.load(f)\n        \n        # Extract the required fields\n        IBSN = data.get(\"IBSN\", \"\")\n        subject = data.get(\"subject\", \"\")\n        topic = data.get(\"topic\", \"\")\n        questions = data.get(\"text\", \"\")\n        figures = [f\"{figure.get('label', '')}: {figure.get('path', '')}\" for figure in data.get(\"figures\", [])]\n        # Ensure options are formatted as [\"A:\",\"B:\",\"C:\",\"D:\"]\n        options = [f\"{key}: {value}\" for key, value in sorted(data.get(\"options\", {}).items())]\n        answers = data.get(\"answers\", \"\")\n\n        # Extract and clean the file name\n        clean_file_name = file_name.replace(\"_en\", \"\").replace(\"_ms\", \"\").replace(\".json\", \"\")\n        \n        # Append a row to the table\n        rows.append({\n            \"FileName\": clean_file_name,\n            \"IBSN\": IBSN,\n            \"Subject\": subject,\n            \"Topics\": topic,\n            \"Questions\": questions,\n            \"Figures\": \", \".join(figures),\n            \"Options\": [f\"{option}\" for option in options],\n            \"Answers\": answers\n        })\n\n# Convert the rows into a Pandas DataFrame\ndf = pd.DataFrame(rows)\n\n# Define the output directory and create it if it doesn't exist\noutput_table_dir = \"/kaggle/working/table\"\nos.makedirs(output_table_dir, exist_ok=True)\n\n# Define the output CSV file path\noutput_csv_path = os.path.join(output_table_dir, \"final_dataset_table_ms.csv\")\n\n# Export the DataFrame to a CSV file\ndf.to_csv(output_csv_path, index=False)\n\n# Display the DataFrame for verification\nprint(df.head())\n\n# Display success message\nprint(f\"Table successfully created and saved to {output_csv_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T12:48:40.986778Z","iopub.execute_input":"2024-12-24T12:48:40.987114Z","iopub.status.idle":"2024-12-24T12:48:41.037175Z","shell.execute_reply.started":"2024-12-24T12:48:40.987084Z","shell.execute_reply":"2024-12-24T12:48:41.036347Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Use for download from Kaggle","metadata":{}},{"cell_type":"code","source":"import shutil\n\n# Define the directory containing the output files\noutput_directory = \"/kaggle/working/table\"\noutput_zip_path = \"Combined_dataset.zip\"\n\n# Create a ZIP file containing all files and folders within the output directory\nshutil.make_archive(output_zip_path.replace(\".zip\", \"\"), 'zip', output_directory)\n\nprint(f\"ZIP file created at: {output_zip_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T12:49:20.736409Z","iopub.execute_input":"2024-12-24T12:49:20.736990Z","iopub.status.idle":"2024-12-24T12:49:20.761944Z","shell.execute_reply.started":"2024-12-24T12:49:20.736952Z","shell.execute_reply":"2024-12-24T12:49:20.761281Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}